{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1.post2\n",
      "Torchvision Version:  0.2.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "trainLabels = pd.read_csv(\"./trainLabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"./train/train_224\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 5\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train/train_224/*'\n",
    "images_list = glob.glob(train_path)\n",
    "images_list = sorted(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_images = 10\n",
    "# def createDataset(images_list,trainLabels):\n",
    "#     images = []\n",
    "#     imlabel = []\n",
    "#     for img in images_list[:number_of_images]:\n",
    "#         images.append(np.array(Image.open(img)))\n",
    "#         fileName = img.split('/')[-1].split('.')[0]\n",
    "#         imlabel_category = np.zeros(5)\n",
    "#         imlabel_category[trainLabels.loc[trainLabels.image==fileName, 'level'].values[0]] = 1\n",
    "#         imlabel.append(imlabel_category)\n",
    "#     images = np.array(images)\n",
    "#     imlabel = np.array(imlabel)\n",
    "#     return images , imlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X , y = createDataset(images_list,trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.from_numpy(X_train)\n",
    "# y_train = torch.from_numpy(y_train)\n",
    "# X_test = torch.from_numpy(X_test)\n",
    "# y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, images_list,trainLables,transform=None):\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "#         self.transform = transforms.Resize(224)\n",
    "        self.images_list = images_list\n",
    "        self.trainLables = trainLables\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        print(len(self.images_list))\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(\"index \",idx)\n",
    "        imgtemp = Image.open(self.images_list[idx])\n",
    "        if self.transform!=None :\n",
    "            imgtemp = self.transform(imgtemp)\n",
    "        img = transforms.ToTensor()(imgtemp)\n",
    "        fileName = self.images_list[idx].split('/')[-1].split('.')[0]\n",
    "        imglabel = torch.zeros(5,dtype=torch.long)\n",
    "        imglabel[self.trainLables.loc[self.trainLables.image==fileName ,'level'].values[0]] = 1\n",
    "        \n",
    "#         xtemp = self.transform(self.X[idx])\n",
    "#         ytemp = self.transform(self.y[idx])\n",
    "        return img ,imglabel\n",
    "\n",
    "class EyeDatasetFromImages(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train_list ,images_test_list = train_test_split(images_list,test_size=0.2,random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders_dict = { 'train' : torch.utils.data.DataLoader(EyeDatasetFromImages(X_train,y_train),batch_size=batch_size) }\n",
    "# dataloaders_dict['val'] = torch.utils.data.DataLoader(EyeDatasetFromImages(X_test,y_test),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dict = { 'train' : torch.utils.data.DataLoader(EyeDataset(images_train_list,trainLabels),batch_size=batch_size) }\n",
    "dataloaders_dict['val'] = torch.utils.data.DataLoader(EyeDataset(images_test_list,trainLabels),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, checkpoint,num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            num_b = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                num_b += 1\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "#                 print(\"input size \",inputs.size())\n",
    "#                 print(\"label size\",labels.size())\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, torch.max(labels,1)[1])\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == torch.max(labels,1)[1])\n",
    "                \n",
    "                if num_b%35==0 :\n",
    "                    print('loss in {} batch is {:.4f}'.format(num_b,loss))\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        }, checkpoint)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_model():\n",
    "    return models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    return model_ft , input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "# model_toy = toy_model()\n",
    "# model_toy.classifier[6] = nn.Linear(4096,num_classes)\n",
    "checkpoint = \"resnet50.checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "# model_toy = model_toy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = model_ft.parameters()\n",
    "# params_to_update = model_toy.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.1789\n",
      "loss in 70 batch is 0.3338\n",
      "loss in 105 batch is 0.3331\n",
      "loss in 140 batch is 0.0903\n",
      "loss in 175 batch is 1.4743\n",
      "loss in 210 batch is 0.3177\n",
      "loss in 245 batch is 1.3052\n",
      "loss in 280 batch is 0.6386\n",
      "loss in 315 batch is 0.3779\n",
      "loss in 350 batch is 0.2563\n",
      "loss in 385 batch is 0.1926\n",
      "loss in 420 batch is 0.2652\n",
      "loss in 455 batch is 2.1361\n",
      "loss in 490 batch is 0.4513\n",
      "loss in 525 batch is 0.3772\n",
      "loss in 560 batch is 0.7516\n",
      "loss in 595 batch is 0.5935\n",
      "loss in 630 batch is 0.3800\n",
      "loss in 665 batch is 0.5735\n",
      "loss in 700 batch is 0.4593\n",
      "loss in 735 batch is 0.7562\n",
      "loss in 770 batch is 0.1595\n",
      "loss in 805 batch is 0.6343\n",
      "loss in 840 batch is 0.4571\n",
      "loss in 875 batch is 0.3624\n",
      "loss in 910 batch is 1.1375\n",
      "loss in 945 batch is 0.6143\n",
      "loss in 980 batch is 0.8383\n",
      "loss in 1015 batch is 0.5657\n",
      "loss in 1050 batch is 0.5734\n",
      "loss in 1085 batch is 0.9962\n",
      "loss in 1120 batch is 0.8102\n",
      "loss in 1155 batch is 0.7825\n",
      "loss in 1190 batch is 0.3564\n",
      "loss in 1225 batch is 0.2479\n",
      "loss in 1260 batch is 0.1819\n",
      "loss in 1295 batch is 0.5627\n",
      "loss in 1330 batch is 0.2454\n",
      "loss in 1365 batch is 1.0810\n",
      "loss in 1400 batch is 0.3056\n",
      "loss in 1435 batch is 0.5156\n",
      "loss in 1470 batch is 0.3567\n",
      "loss in 1505 batch is 0.5276\n",
      "loss in 1540 batch is 0.2610\n",
      "loss in 1575 batch is 0.4339\n",
      "loss in 1610 batch is 0.8762\n",
      "loss in 1645 batch is 0.4164\n",
      "loss in 1680 batch is 0.3371\n",
      "loss in 1715 batch is 0.5799\n",
      "loss in 1750 batch is 0.3462\n",
      "loss in 1785 batch is 0.3433\n",
      "loss in 1820 batch is 0.0810\n",
      "loss in 1855 batch is 0.4287\n",
      "loss in 1890 batch is 0.6144\n",
      "loss in 1925 batch is 0.2428\n",
      "loss in 1960 batch is 0.3828\n",
      "loss in 1995 batch is 0.2326\n",
      "loss in 2030 batch is 1.0833\n",
      "loss in 2065 batch is 0.5914\n",
      "loss in 2100 batch is 0.3790\n",
      "loss in 2135 batch is 0.1387\n",
      "loss in 2170 batch is 0.2870\n",
      "loss in 2205 batch is 0.9395\n",
      "loss in 2240 batch is 0.3831\n",
      "loss in 2275 batch is 0.3802\n",
      "loss in 2310 batch is 0.1611\n",
      "loss in 2345 batch is 0.4889\n",
      "loss in 2380 batch is 0.4342\n",
      "loss in 2415 batch is 0.1139\n",
      "loss in 2450 batch is 0.2976\n",
      "loss in 2485 batch is 0.3848\n",
      "loss in 2520 batch is 0.2988\n",
      "loss in 2555 batch is 0.4338\n",
      "loss in 2590 batch is 1.1814\n",
      "loss in 2625 batch is 0.2500\n",
      "loss in 2660 batch is 1.1728\n",
      "loss in 2695 batch is 0.1356\n",
      "loss in 2730 batch is 0.7362\n",
      "loss in 2765 batch is 1.5552\n",
      "loss in 2800 batch is 0.7256\n",
      "loss in 2835 batch is 0.9788\n",
      "loss in 2870 batch is 0.8410\n",
      "loss in 2905 batch is 0.1112\n",
      "loss in 2940 batch is 0.5896\n",
      "loss in 2975 batch is 0.5923\n",
      "loss in 3010 batch is 0.3347\n",
      "loss in 3045 batch is 0.3376\n",
      "loss in 3080 batch is 0.8391\n",
      "loss in 3115 batch is 0.3706\n",
      "loss in 3150 batch is 0.3511\n",
      "loss in 3185 batch is 0.4861\n",
      "loss in 3220 batch is 1.4262\n",
      "loss in 3255 batch is 0.7942\n",
      "loss in 3290 batch is 0.8102\n",
      "loss in 3325 batch is 0.1729\n",
      "loss in 3360 batch is 0.1233\n",
      "loss in 3395 batch is 0.3285\n",
      "loss in 3430 batch is 1.2935\n",
      "loss in 3465 batch is 0.3914\n",
      "loss in 3500 batch is 0.1680\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.5747 Acc: 0.8066\n",
      "7026\n",
      "loss in 35 batch is 0.5954\n",
      "loss in 70 batch is 0.5701\n",
      "loss in 105 batch is 0.5182\n",
      "loss in 140 batch is 0.2689\n",
      "loss in 175 batch is 0.7659\n",
      "loss in 210 batch is 0.5074\n",
      "loss in 245 batch is 0.2918\n",
      "loss in 280 batch is 0.8004\n",
      "loss in 315 batch is 0.2413\n",
      "loss in 350 batch is 0.3724\n",
      "loss in 385 batch is 0.9091\n",
      "loss in 420 batch is 0.5409\n",
      "loss in 455 batch is 1.3125\n",
      "loss in 490 batch is 0.2746\n",
      "loss in 525 batch is 1.0211\n",
      "loss in 560 batch is 0.2318\n",
      "loss in 595 batch is 0.7424\n",
      "loss in 630 batch is 0.7501\n",
      "loss in 665 batch is 1.3066\n",
      "loss in 700 batch is 0.9732\n",
      "loss in 735 batch is 0.7966\n",
      "loss in 770 batch is 0.7114\n",
      "loss in 805 batch is 0.2315\n",
      "loss in 840 batch is 0.4717\n",
      "loss in 875 batch is 0.2210\n",
      "7026\n",
      "7026\n",
      "val Loss: 0.6939 Acc: 0.7811\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.1965\n",
      "loss in 70 batch is 0.4080\n",
      "loss in 105 batch is 0.9682\n",
      "loss in 140 batch is 0.2702\n",
      "loss in 175 batch is 1.5042\n",
      "loss in 210 batch is 0.3259\n",
      "loss in 245 batch is 1.8003\n",
      "loss in 280 batch is 0.5901\n",
      "loss in 315 batch is 0.4350\n",
      "loss in 350 batch is 0.1254\n",
      "loss in 385 batch is 0.1104\n",
      "loss in 420 batch is 0.1760\n",
      "loss in 455 batch is 1.7430\n",
      "loss in 490 batch is 0.2994\n",
      "loss in 525 batch is 0.2070\n",
      "loss in 560 batch is 0.6315\n",
      "loss in 595 batch is 0.4329\n",
      "loss in 630 batch is 0.4141\n",
      "loss in 665 batch is 0.5750\n",
      "loss in 700 batch is 0.5893\n",
      "loss in 735 batch is 0.7504\n",
      "loss in 770 batch is 0.1496\n",
      "loss in 805 batch is 0.4623\n",
      "loss in 840 batch is 0.3597\n",
      "loss in 875 batch is 0.5545\n",
      "loss in 910 batch is 0.9720\n",
      "loss in 945 batch is 0.5554\n",
      "loss in 980 batch is 0.5487\n",
      "loss in 1015 batch is 0.5231\n",
      "loss in 1050 batch is 0.5904\n",
      "loss in 1085 batch is 0.2932\n",
      "loss in 1120 batch is 0.4726\n",
      "loss in 1155 batch is 0.6237\n",
      "loss in 1190 batch is 0.2024\n",
      "loss in 1225 batch is 0.0720\n",
      "loss in 1260 batch is 0.3678\n",
      "loss in 1295 batch is 0.2872\n",
      "loss in 1330 batch is 0.2379\n",
      "loss in 1365 batch is 1.0255\n",
      "loss in 1400 batch is 0.2048\n",
      "loss in 1435 batch is 0.4755\n",
      "loss in 1470 batch is 0.5665\n",
      "loss in 1505 batch is 0.7743\n",
      "loss in 1540 batch is 0.2142\n",
      "loss in 1575 batch is 0.4362\n",
      "loss in 1610 batch is 0.9165\n",
      "loss in 1645 batch is 0.3645\n",
      "loss in 1680 batch is 0.5045\n",
      "loss in 1715 batch is 0.5422\n",
      "loss in 1750 batch is 0.4400\n",
      "loss in 1785 batch is 0.3545\n",
      "loss in 1820 batch is 0.0974\n",
      "loss in 1855 batch is 0.2708\n",
      "loss in 1890 batch is 0.5099\n",
      "loss in 1925 batch is 0.2825\n",
      "loss in 1960 batch is 0.2809\n",
      "loss in 1995 batch is 0.1793\n",
      "loss in 2030 batch is 0.9715\n",
      "loss in 2065 batch is 0.4056\n",
      "loss in 2100 batch is 0.3670\n",
      "loss in 2135 batch is 0.1150\n",
      "loss in 2170 batch is 0.2012\n",
      "loss in 2205 batch is 0.7298\n",
      "loss in 2240 batch is 0.1895\n",
      "loss in 2275 batch is 0.6170\n",
      "loss in 2310 batch is 0.2088\n",
      "loss in 2345 batch is 0.4772\n",
      "loss in 2380 batch is 0.4625\n",
      "loss in 2415 batch is 0.1540\n",
      "loss in 2450 batch is 0.1068\n",
      "loss in 2485 batch is 0.3095\n",
      "loss in 2520 batch is 0.1780\n",
      "loss in 2555 batch is 0.3158\n",
      "loss in 2590 batch is 1.0946\n",
      "loss in 2625 batch is 0.2565\n",
      "loss in 2660 batch is 1.3141\n",
      "loss in 2695 batch is 0.1424\n",
      "loss in 2730 batch is 0.4824\n",
      "loss in 2765 batch is 1.2501\n",
      "loss in 2800 batch is 0.3576\n",
      "loss in 2835 batch is 0.7448\n",
      "loss in 2870 batch is 0.4341\n",
      "loss in 2905 batch is 0.3257\n",
      "loss in 2940 batch is 0.5819\n",
      "loss in 2975 batch is 0.3589\n",
      "loss in 3010 batch is 0.2424\n",
      "loss in 3045 batch is 0.2694\n",
      "loss in 3080 batch is 0.8513\n",
      "loss in 3115 batch is 0.0852\n",
      "loss in 3150 batch is 0.2854\n",
      "loss in 3185 batch is 0.5237\n",
      "loss in 3220 batch is 1.2630\n",
      "loss in 3255 batch is 0.9653\n",
      "loss in 3290 batch is 0.6876\n",
      "loss in 3325 batch is 0.3030\n",
      "loss in 3360 batch is 0.0806\n",
      "loss in 3395 batch is 0.1831\n",
      "loss in 3430 batch is 1.3176\n",
      "loss in 3465 batch is 0.6032\n",
      "loss in 3500 batch is 0.1031\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.5148 Acc: 0.8204\n",
      "7026\n",
      "loss in 35 batch is 0.5522\n",
      "loss in 70 batch is 0.3112\n",
      "loss in 105 batch is 0.5528\n",
      "loss in 140 batch is 0.1815\n",
      "loss in 175 batch is 0.7289\n",
      "loss in 210 batch is 0.4761\n",
      "loss in 245 batch is 0.2815\n",
      "loss in 280 batch is 0.6496\n",
      "loss in 315 batch is 0.4788\n",
      "loss in 350 batch is 0.7005\n",
      "loss in 385 batch is 0.7168\n",
      "loss in 420 batch is 0.3803\n",
      "loss in 455 batch is 1.2260\n",
      "loss in 490 batch is 0.2546\n",
      "loss in 525 batch is 0.7339\n",
      "loss in 560 batch is 0.4625\n",
      "loss in 595 batch is 0.5783\n",
      "loss in 630 batch is 0.6690\n",
      "loss in 665 batch is 1.7109\n",
      "loss in 700 batch is 0.9276\n",
      "loss in 735 batch is 1.1550\n",
      "loss in 770 batch is 0.8459\n",
      "loss in 805 batch is 0.5856\n",
      "loss in 840 batch is 0.3583\n",
      "loss in 875 batch is 0.3228\n",
      "7026\n",
      "7026\n",
      "val Loss: 0.7899 Acc: 0.7731\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.2511\n",
      "loss in 70 batch is 0.5022\n",
      "loss in 105 batch is 0.5480\n",
      "loss in 140 batch is 0.0897\n",
      "loss in 175 batch is 1.5486\n",
      "loss in 210 batch is 0.2476\n",
      "loss in 245 batch is 1.4222\n",
      "loss in 280 batch is 0.4236\n",
      "loss in 315 batch is 0.5167\n",
      "loss in 350 batch is 0.1826\n",
      "loss in 385 batch is 0.1768\n",
      "loss in 420 batch is 0.2106\n",
      "loss in 455 batch is 1.9800\n",
      "loss in 490 batch is 0.4395\n",
      "loss in 525 batch is 0.1966\n",
      "loss in 560 batch is 0.9007\n",
      "loss in 595 batch is 0.5966\n",
      "loss in 630 batch is 0.3177\n",
      "loss in 665 batch is 0.5501\n",
      "loss in 700 batch is 0.4626\n",
      "loss in 735 batch is 0.5673\n",
      "loss in 770 batch is 0.1499\n",
      "loss in 805 batch is 0.3363\n",
      "loss in 840 batch is 0.3664\n",
      "loss in 875 batch is 0.2801\n",
      "loss in 910 batch is 0.7697\n",
      "loss in 945 batch is 0.5240\n",
      "loss in 980 batch is 0.3100\n",
      "loss in 1015 batch is 0.3202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in 1050 batch is 0.2884\n",
      "loss in 1085 batch is 0.4522\n",
      "loss in 1120 batch is 0.3813\n",
      "loss in 1155 batch is 0.3633\n",
      "loss in 1190 batch is 0.7089\n",
      "loss in 1225 batch is 0.6529\n",
      "loss in 1260 batch is 0.0824\n",
      "loss in 1295 batch is 0.6711\n",
      "loss in 1330 batch is 0.2329\n",
      "loss in 1365 batch is 1.1268\n",
      "loss in 1400 batch is 0.3369\n",
      "loss in 1435 batch is 0.7596\n",
      "loss in 1470 batch is 0.2798\n",
      "loss in 1505 batch is 0.4747\n",
      "loss in 1540 batch is 0.1766\n",
      "loss in 1575 batch is 0.4280\n",
      "loss in 1610 batch is 0.8317\n",
      "loss in 1645 batch is 0.2391\n",
      "loss in 1680 batch is 0.5607\n",
      "loss in 1715 batch is 0.4083\n",
      "loss in 1750 batch is 0.1516\n",
      "loss in 1785 batch is 0.3488\n",
      "loss in 1820 batch is 0.1136\n",
      "loss in 1855 batch is 0.3979\n",
      "loss in 1890 batch is 0.5445\n",
      "loss in 1925 batch is 0.3802\n",
      "loss in 1960 batch is 1.3015\n",
      "loss in 1995 batch is 0.5171\n",
      "loss in 2030 batch is 0.8531\n",
      "loss in 2065 batch is 0.4650\n",
      "loss in 2100 batch is 0.5465\n",
      "loss in 2135 batch is 0.1783\n",
      "loss in 2170 batch is 0.4274\n",
      "loss in 2205 batch is 0.6838\n",
      "loss in 2240 batch is 0.1753\n",
      "loss in 2275 batch is 0.2817\n",
      "loss in 2310 batch is 0.2695\n",
      "loss in 2345 batch is 0.4404\n",
      "loss in 2380 batch is 0.2251\n",
      "loss in 2415 batch is 0.2823\n",
      "loss in 2450 batch is 0.1218\n",
      "loss in 2485 batch is 0.4364\n",
      "loss in 2520 batch is 0.1678\n",
      "loss in 2555 batch is 0.3354\n",
      "loss in 2590 batch is 0.8908\n",
      "loss in 2625 batch is 0.3637\n",
      "loss in 2660 batch is 0.9343\n",
      "loss in 2695 batch is 0.1353\n",
      "loss in 2730 batch is 0.5883\n",
      "loss in 2765 batch is 0.8450\n",
      "loss in 2800 batch is 0.8696\n",
      "loss in 2835 batch is 0.4370\n",
      "loss in 2870 batch is 0.6934\n",
      "loss in 2905 batch is 0.1032\n",
      "loss in 2940 batch is 0.4068\n",
      "loss in 2975 batch is 0.4503\n",
      "loss in 3010 batch is 0.2880\n",
      "loss in 3045 batch is 0.1161\n",
      "loss in 3080 batch is 0.6233\n",
      "loss in 3115 batch is 0.2739\n",
      "loss in 3150 batch is 0.1549\n",
      "loss in 3185 batch is 0.4178\n",
      "loss in 3220 batch is 0.6806\n",
      "loss in 3255 batch is 0.8118\n",
      "loss in 3290 batch is 0.6872\n",
      "loss in 3325 batch is 0.1327\n",
      "loss in 3360 batch is 0.0643\n",
      "loss in 3395 batch is 0.1477\n",
      "loss in 3430 batch is 1.1820\n",
      "loss in 3465 batch is 0.3696\n",
      "loss in 3500 batch is 0.1724\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.4648 Acc: 0.8326\n",
      "7026\n",
      "loss in 35 batch is 0.8287\n",
      "loss in 70 batch is 0.6545\n",
      "loss in 105 batch is 0.6603\n",
      "loss in 140 batch is 0.2855\n",
      "loss in 175 batch is 1.2279\n",
      "loss in 210 batch is 0.5362\n",
      "loss in 245 batch is 0.5464\n",
      "loss in 280 batch is 0.5877\n",
      "loss in 315 batch is 0.2872\n",
      "loss in 350 batch is 0.3969\n",
      "loss in 385 batch is 1.3332\n",
      "loss in 420 batch is 0.4632\n",
      "loss in 455 batch is 1.3340\n",
      "loss in 490 batch is 0.3460\n",
      "loss in 525 batch is 1.2286\n",
      "loss in 560 batch is 0.5694\n",
      "loss in 595 batch is 0.7223\n",
      "loss in 630 batch is 0.7143\n",
      "loss in 665 batch is 1.4331\n",
      "loss in 700 batch is 1.4018\n",
      "loss in 735 batch is 1.3882\n",
      "loss in 770 batch is 0.9490\n",
      "loss in 805 batch is 0.4112\n",
      "loss in 840 batch is 0.4700\n",
      "loss in 875 batch is 0.2969\n",
      "7026\n",
      "7026\n",
      "val Loss: 0.7841 Acc: 0.7541\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.2742\n",
      "loss in 70 batch is 0.7197\n",
      "loss in 105 batch is 0.5224\n",
      "loss in 140 batch is 0.1681\n",
      "loss in 175 batch is 1.2757\n",
      "loss in 210 batch is 0.1432\n",
      "loss in 245 batch is 1.2057\n",
      "loss in 280 batch is 0.2378\n",
      "loss in 315 batch is 0.2614\n",
      "loss in 350 batch is 0.2101\n",
      "loss in 385 batch is 0.0725\n",
      "loss in 420 batch is 0.1855\n",
      "loss in 455 batch is 1.2709\n",
      "loss in 490 batch is 0.0975\n",
      "loss in 525 batch is 0.4017\n",
      "loss in 560 batch is 0.7640\n",
      "loss in 595 batch is 0.5626\n",
      "loss in 630 batch is 0.4586\n",
      "loss in 665 batch is 0.3671\n",
      "loss in 700 batch is 0.3615\n",
      "loss in 735 batch is 0.6325\n",
      "loss in 770 batch is 0.1130\n",
      "loss in 805 batch is 0.2849\n",
      "loss in 840 batch is 0.4071\n",
      "loss in 875 batch is 0.2820\n",
      "loss in 910 batch is 0.9861\n",
      "loss in 945 batch is 0.1391\n",
      "loss in 980 batch is 0.4765\n",
      "loss in 1015 batch is 0.1584\n",
      "loss in 1050 batch is 0.4189\n",
      "loss in 1085 batch is 0.4036\n",
      "loss in 1120 batch is 0.3524\n",
      "loss in 1155 batch is 0.3492\n",
      "loss in 1190 batch is 0.3003\n",
      "loss in 1225 batch is 0.0887\n",
      "loss in 1260 batch is 0.1378\n",
      "loss in 1295 batch is 0.2777\n",
      "loss in 1330 batch is 0.1541\n",
      "loss in 1365 batch is 0.6520\n",
      "loss in 1400 batch is 0.2122\n",
      "loss in 1435 batch is 0.3575\n",
      "loss in 1470 batch is 0.1419\n",
      "loss in 1505 batch is 0.4018\n",
      "loss in 1540 batch is 0.1956\n",
      "loss in 1575 batch is 0.3856\n",
      "loss in 1610 batch is 0.5380\n",
      "loss in 1645 batch is 0.1920\n",
      "loss in 1680 batch is 0.2106\n",
      "loss in 1715 batch is 0.3188\n",
      "loss in 1750 batch is 0.2282\n",
      "loss in 1785 batch is 0.2881\n",
      "loss in 1820 batch is 0.0765\n",
      "loss in 1855 batch is 0.1101\n",
      "loss in 1890 batch is 0.2704\n",
      "loss in 1925 batch is 0.2208\n",
      "loss in 1960 batch is 0.2455\n",
      "loss in 1995 batch is 0.2386\n",
      "loss in 2030 batch is 0.7584\n",
      "loss in 2065 batch is 0.2500\n",
      "loss in 2100 batch is 0.3629\n",
      "loss in 2135 batch is 0.1654\n",
      "loss in 2170 batch is 0.1044\n",
      "loss in 2205 batch is 0.3661\n",
      "loss in 2240 batch is 0.3631\n",
      "loss in 2275 batch is 0.2568\n",
      "loss in 2310 batch is 0.4915\n",
      "loss in 2345 batch is 0.4471\n",
      "loss in 2380 batch is 0.4362\n",
      "loss in 2415 batch is 0.1378\n",
      "loss in 2450 batch is 0.1398\n",
      "loss in 2485 batch is 0.3249\n",
      "loss in 2520 batch is 0.2454\n",
      "loss in 2555 batch is 0.3095\n",
      "loss in 2590 batch is 0.7240\n",
      "loss in 2625 batch is 0.2998\n",
      "loss in 2660 batch is 0.6552\n",
      "loss in 2695 batch is 0.1003\n",
      "loss in 2730 batch is 0.5493\n",
      "loss in 2765 batch is 0.9089\n",
      "loss in 2800 batch is 0.3277\n",
      "loss in 2835 batch is 0.5836\n",
      "loss in 2870 batch is 0.3674\n",
      "loss in 2905 batch is 0.0843\n",
      "loss in 2940 batch is 0.4988\n",
      "loss in 2975 batch is 0.4040\n",
      "loss in 3010 batch is 0.2837\n",
      "loss in 3045 batch is 0.1544\n",
      "loss in 3080 batch is 0.4496\n",
      "loss in 3115 batch is 0.1654\n",
      "loss in 3150 batch is 0.1942\n",
      "loss in 3185 batch is 0.2933\n",
      "loss in 3220 batch is 0.5866\n",
      "loss in 3255 batch is 0.6028\n",
      "loss in 3290 batch is 0.7109\n",
      "loss in 3325 batch is 0.1870\n",
      "loss in 3360 batch is 0.0960\n",
      "loss in 3395 batch is 0.2573\n",
      "loss in 3430 batch is 1.2349\n",
      "loss in 3465 batch is 0.9047\n",
      "loss in 3500 batch is 0.1896\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.4183 Acc: 0.8480\n",
      "7026\n",
      "loss in 35 batch is 1.0020\n",
      "loss in 70 batch is 0.2565\n",
      "loss in 105 batch is 0.9901\n",
      "loss in 140 batch is 0.2098\n",
      "loss in 175 batch is 1.2136\n",
      "loss in 210 batch is 0.5990\n",
      "loss in 245 batch is 0.6449\n",
      "loss in 280 batch is 0.6818\n",
      "loss in 315 batch is 0.1444\n",
      "loss in 350 batch is 0.6492\n",
      "loss in 385 batch is 1.6688\n",
      "loss in 420 batch is 0.3252\n",
      "loss in 455 batch is 1.6474\n",
      "loss in 490 batch is 0.3436\n",
      "loss in 525 batch is 0.8018\n",
      "loss in 560 batch is 0.4617\n",
      "loss in 595 batch is 0.4660\n",
      "loss in 630 batch is 0.7088\n",
      "loss in 665 batch is 1.9001\n",
      "loss in 700 batch is 1.1159\n",
      "loss in 735 batch is 0.5879\n",
      "loss in 770 batch is 1.5420\n",
      "loss in 805 batch is 0.6821\n",
      "loss in 840 batch is 0.2038\n",
      "loss in 875 batch is 0.2154\n",
      "7026\n",
      "7026\n",
      "val Loss: 0.8556 Acc: 0.7445\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.5244\n",
      "loss in 70 batch is 0.7526\n",
      "loss in 105 batch is 0.6358\n",
      "loss in 140 batch is 0.2333\n",
      "loss in 175 batch is 0.7018\n",
      "loss in 210 batch is 0.0610\n",
      "loss in 245 batch is 1.3209\n",
      "loss in 280 batch is 0.2575\n",
      "loss in 315 batch is 0.2542\n",
      "loss in 350 batch is 0.1961\n",
      "loss in 385 batch is 0.0690\n",
      "loss in 420 batch is 0.1249\n",
      "loss in 455 batch is 1.1631\n",
      "loss in 490 batch is 0.5572\n",
      "loss in 525 batch is 0.3925\n",
      "loss in 560 batch is 0.2811\n",
      "loss in 595 batch is 0.2813\n",
      "loss in 630 batch is 0.4043\n",
      "loss in 665 batch is 0.9003\n",
      "loss in 700 batch is 0.4826\n",
      "loss in 735 batch is 0.4484\n",
      "loss in 770 batch is 0.0619\n",
      "loss in 805 batch is 0.3217\n",
      "loss in 840 batch is 0.3857\n",
      "loss in 875 batch is 0.1379\n",
      "loss in 910 batch is 0.4126\n",
      "loss in 945 batch is 0.1319\n",
      "loss in 980 batch is 0.0752\n",
      "loss in 1015 batch is 0.1939\n",
      "loss in 1050 batch is 0.2526\n",
      "loss in 1085 batch is 0.0798\n",
      "loss in 1120 batch is 0.3545\n",
      "loss in 1155 batch is 0.3203\n",
      "loss in 1190 batch is 0.0517\n",
      "loss in 1225 batch is 0.0386\n",
      "loss in 1260 batch is 0.1232\n",
      "loss in 1295 batch is 0.1609\n",
      "loss in 1330 batch is 0.2330\n",
      "loss in 1365 batch is 1.5462\n",
      "loss in 1400 batch is 0.3401\n",
      "loss in 1435 batch is 0.5454\n",
      "loss in 1470 batch is 0.1353\n",
      "loss in 1505 batch is 0.3187\n",
      "loss in 1540 batch is 0.3507\n",
      "loss in 1575 batch is 0.1724\n",
      "loss in 1610 batch is 0.4256\n",
      "loss in 1645 batch is 0.1469\n",
      "loss in 1680 batch is 0.2462\n",
      "loss in 1715 batch is 0.4038\n",
      "loss in 1750 batch is 0.2021\n",
      "loss in 1785 batch is 0.1948\n",
      "loss in 1820 batch is 0.1459\n",
      "loss in 1855 batch is 0.5875\n",
      "loss in 1890 batch is 1.2575\n",
      "loss in 1925 batch is 0.1399\n",
      "loss in 1960 batch is 0.4333\n",
      "loss in 1995 batch is 0.3244\n",
      "loss in 2030 batch is 0.4172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in 2065 batch is 0.6153\n",
      "loss in 2100 batch is 0.2590\n",
      "loss in 2135 batch is 0.1000\n",
      "loss in 2170 batch is 0.1725\n",
      "loss in 2205 batch is 0.3837\n",
      "loss in 2240 batch is 0.0793\n",
      "loss in 2275 batch is 0.2871\n",
      "loss in 2310 batch is 0.3504\n",
      "loss in 2345 batch is 0.1811\n",
      "loss in 2380 batch is 0.1371\n",
      "loss in 2415 batch is 0.1048\n",
      "loss in 2450 batch is 0.1579\n",
      "loss in 2485 batch is 0.1213\n",
      "loss in 2520 batch is 0.2082\n",
      "loss in 2555 batch is 0.2396\n",
      "loss in 2590 batch is 0.5573\n",
      "loss in 2625 batch is 0.3212\n",
      "loss in 2660 batch is 0.7371\n",
      "loss in 2695 batch is 0.1142\n",
      "loss in 2730 batch is 0.5792\n",
      "loss in 2765 batch is 0.6485\n",
      "loss in 2800 batch is 0.6914\n",
      "loss in 2835 batch is 0.3254\n",
      "loss in 2870 batch is 0.1818\n",
      "loss in 2905 batch is 0.1503\n",
      "loss in 2940 batch is 0.3467\n",
      "loss in 2975 batch is 0.3286\n",
      "loss in 3010 batch is 0.1244\n",
      "loss in 3045 batch is 0.0516\n",
      "loss in 3080 batch is 0.2303\n",
      "loss in 3115 batch is 0.1224\n",
      "loss in 3150 batch is 0.2072\n",
      "loss in 3185 batch is 0.6988\n",
      "loss in 3220 batch is 0.7992\n",
      "loss in 3255 batch is 0.2198\n",
      "loss in 3290 batch is 0.2751\n",
      "loss in 3325 batch is 0.0765\n",
      "loss in 3360 batch is 0.0840\n",
      "loss in 3395 batch is 0.1513\n",
      "loss in 3430 batch is 0.7947\n",
      "loss in 3465 batch is 0.3014\n",
      "loss in 3500 batch is 0.1299\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.3699 Acc: 0.8640\n",
      "7026\n",
      "loss in 35 batch is 0.8471\n",
      "loss in 70 batch is 0.2172\n",
      "loss in 105 batch is 0.8295\n",
      "loss in 140 batch is 0.4537\n",
      "loss in 175 batch is 1.2877\n",
      "loss in 210 batch is 1.1646\n",
      "loss in 245 batch is 0.5624\n",
      "loss in 280 batch is 0.9844\n",
      "loss in 315 batch is 0.4010\n",
      "loss in 350 batch is 0.6351\n",
      "loss in 385 batch is 1.1255\n",
      "loss in 420 batch is 0.4876\n",
      "loss in 455 batch is 1.8948\n",
      "loss in 490 batch is 0.3548\n",
      "loss in 525 batch is 1.0813\n",
      "loss in 560 batch is 1.2071\n",
      "loss in 595 batch is 0.6556\n",
      "loss in 630 batch is 0.5181\n",
      "loss in 665 batch is 2.6218\n",
      "loss in 700 batch is 1.3226\n",
      "loss in 735 batch is 0.6136\n",
      "loss in 770 batch is 1.0964\n",
      "loss in 805 batch is 0.1337\n",
      "loss in 840 batch is 0.2143\n",
      "loss in 875 batch is 0.3966\n",
      "7026\n",
      "7026\n",
      "val Loss: 0.9144 Acc: 0.7356\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.3482\n",
      "loss in 70 batch is 0.3318\n",
      "loss in 105 batch is 0.4842\n",
      "loss in 140 batch is 0.0431\n",
      "loss in 175 batch is 1.0787\n",
      "loss in 210 batch is 0.1143\n",
      "loss in 245 batch is 1.2211\n",
      "loss in 280 batch is 0.0761\n",
      "loss in 315 batch is 0.1506\n",
      "loss in 350 batch is 0.2900\n",
      "loss in 385 batch is 0.1392\n",
      "loss in 420 batch is 0.0547\n",
      "loss in 455 batch is 0.7631\n",
      "loss in 490 batch is 0.3910\n",
      "loss in 525 batch is 0.2989\n",
      "loss in 560 batch is 0.8603\n",
      "loss in 595 batch is 0.5958\n",
      "loss in 630 batch is 0.0910\n",
      "loss in 665 batch is 0.6820\n",
      "loss in 700 batch is 0.2209\n",
      "loss in 735 batch is 0.5536\n",
      "loss in 770 batch is 0.0559\n",
      "loss in 805 batch is 0.1241\n",
      "loss in 840 batch is 0.2588\n",
      "loss in 875 batch is 0.1989\n",
      "loss in 910 batch is 0.5613\n",
      "loss in 945 batch is 0.2984\n",
      "loss in 980 batch is 0.4092\n",
      "loss in 1015 batch is 0.2353\n",
      "loss in 1050 batch is 0.0250\n",
      "loss in 1085 batch is 0.1379\n",
      "loss in 1120 batch is 0.4830\n",
      "loss in 1155 batch is 0.1856\n",
      "loss in 1190 batch is 0.3183\n",
      "loss in 1225 batch is 0.0329\n",
      "loss in 1260 batch is 0.0440\n",
      "loss in 1295 batch is 0.2600\n",
      "loss in 1330 batch is 0.1671\n",
      "loss in 1365 batch is 0.6749\n",
      "loss in 1400 batch is 0.4853\n",
      "loss in 1435 batch is 0.3556\n",
      "loss in 1470 batch is 0.0704\n",
      "loss in 1505 batch is 0.1688\n",
      "loss in 1540 batch is 0.2442\n",
      "loss in 1575 batch is 0.2131\n",
      "loss in 1610 batch is 0.2350\n",
      "loss in 1645 batch is 0.4995\n",
      "loss in 1680 batch is 0.1385\n",
      "loss in 1715 batch is 0.0512\n",
      "loss in 1750 batch is 0.2793\n",
      "loss in 1785 batch is 0.4440\n",
      "loss in 1820 batch is 0.1384\n",
      "loss in 1855 batch is 0.1800\n",
      "loss in 1890 batch is 0.1688\n",
      "loss in 1925 batch is 0.0659\n",
      "loss in 1960 batch is 0.2585\n",
      "loss in 1995 batch is 0.4604\n",
      "loss in 2030 batch is 0.5039\n",
      "loss in 2065 batch is 0.4695\n",
      "loss in 2100 batch is 0.1719\n",
      "loss in 2135 batch is 0.1229\n",
      "loss in 2170 batch is 0.1879\n",
      "loss in 2205 batch is 0.1922\n",
      "loss in 2240 batch is 0.1847\n",
      "loss in 2275 batch is 0.0865\n",
      "loss in 2310 batch is 0.2078\n",
      "loss in 2345 batch is 0.2035\n",
      "loss in 2380 batch is 0.2616\n",
      "loss in 2415 batch is 0.0515\n",
      "loss in 2450 batch is 0.0804\n",
      "loss in 2485 batch is 0.0668\n",
      "loss in 2520 batch is 0.2059\n",
      "loss in 2555 batch is 0.0999\n",
      "loss in 2590 batch is 0.6479\n",
      "loss in 2625 batch is 0.2411\n",
      "loss in 2660 batch is 0.7042\n",
      "loss in 2695 batch is 0.0491\n",
      "loss in 2730 batch is 0.5013\n",
      "loss in 2765 batch is 0.5430\n",
      "loss in 2800 batch is 0.2863\n",
      "loss in 2835 batch is 0.2793\n",
      "loss in 2870 batch is 0.1337\n",
      "loss in 2905 batch is 0.3392\n",
      "loss in 2940 batch is 0.4230\n",
      "loss in 2975 batch is 0.2851\n",
      "loss in 3010 batch is 0.2708\n",
      "loss in 3045 batch is 0.1343\n",
      "loss in 3080 batch is 0.1599\n",
      "loss in 3115 batch is 0.0284\n",
      "loss in 3150 batch is 0.1084\n",
      "loss in 3185 batch is 0.0688\n",
      "loss in 3220 batch is 0.9691\n",
      "loss in 3255 batch is 0.1092\n",
      "loss in 3290 batch is 0.6716\n",
      "loss in 3325 batch is 0.0714\n",
      "loss in 3360 batch is 0.3550\n",
      "loss in 3395 batch is 0.0957\n",
      "loss in 3430 batch is 0.7728\n",
      "loss in 3465 batch is 0.2178\n",
      "loss in 3500 batch is 0.0692\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.3163 Acc: 0.8801\n",
      "7026\n",
      "loss in 35 batch is 1.2462\n",
      "loss in 70 batch is 0.2754\n",
      "loss in 105 batch is 1.8491\n",
      "loss in 140 batch is 0.2440\n",
      "loss in 175 batch is 1.8119\n",
      "loss in 210 batch is 0.7065\n",
      "loss in 245 batch is 0.3655\n",
      "loss in 280 batch is 1.0677\n",
      "loss in 315 batch is 0.3523\n",
      "loss in 350 batch is 1.0201\n",
      "loss in 385 batch is 0.7038\n",
      "loss in 420 batch is 0.3960\n",
      "loss in 455 batch is 2.0391\n",
      "loss in 490 batch is 0.3166\n",
      "loss in 525 batch is 0.5530\n",
      "loss in 560 batch is 1.1480\n",
      "loss in 595 batch is 0.9003\n",
      "loss in 630 batch is 0.6694\n",
      "loss in 665 batch is 2.7199\n",
      "loss in 700 batch is 1.1818\n",
      "loss in 735 batch is 0.3850\n",
      "loss in 770 batch is 1.1575\n",
      "loss in 805 batch is 1.5167\n",
      "loss in 840 batch is 0.4044\n",
      "loss in 875 batch is 0.2664\n",
      "7026\n",
      "7026\n",
      "val Loss: 1.0684 Acc: 0.6783\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.0223\n",
      "loss in 70 batch is 0.7296\n",
      "loss in 105 batch is 0.0781\n",
      "loss in 140 batch is 0.2164\n",
      "loss in 175 batch is 0.6095\n",
      "loss in 210 batch is 0.0598\n",
      "loss in 245 batch is 1.1042\n",
      "loss in 280 batch is 0.3314\n",
      "loss in 315 batch is 0.2844\n",
      "loss in 350 batch is 0.4293\n",
      "loss in 385 batch is 0.2202\n",
      "loss in 420 batch is 0.2009\n",
      "loss in 455 batch is 0.3803\n",
      "loss in 490 batch is 0.0906\n",
      "loss in 525 batch is 0.0853\n",
      "loss in 560 batch is 0.1763\n",
      "loss in 595 batch is 0.0681\n",
      "loss in 630 batch is 0.3831\n",
      "loss in 665 batch is 0.4141\n",
      "loss in 700 batch is 0.3696\n",
      "loss in 735 batch is 0.2017\n",
      "loss in 770 batch is 0.0758\n",
      "loss in 805 batch is 0.7914\n",
      "loss in 840 batch is 0.1208\n",
      "loss in 875 batch is 0.0666\n",
      "loss in 910 batch is 0.5268\n",
      "loss in 945 batch is 0.1016\n",
      "loss in 980 batch is 0.1658\n",
      "loss in 1015 batch is 0.0437\n",
      "loss in 1050 batch is 0.1122\n",
      "loss in 1085 batch is 0.1002\n",
      "loss in 1120 batch is 0.4898\n",
      "loss in 1155 batch is 0.9570\n",
      "loss in 1190 batch is 0.2023\n",
      "loss in 1225 batch is 0.0598\n",
      "loss in 1260 batch is 0.0625\n",
      "loss in 1295 batch is 0.8507\n",
      "loss in 1330 batch is 0.2831\n",
      "loss in 1365 batch is 0.1609\n",
      "loss in 1400 batch is 0.0818\n",
      "loss in 1435 batch is 0.3990\n",
      "loss in 1470 batch is 0.4726\n",
      "loss in 1505 batch is 0.2188\n",
      "loss in 1540 batch is 0.2125\n",
      "loss in 1575 batch is 0.0371\n",
      "loss in 1610 batch is 0.1833\n",
      "loss in 1645 batch is 0.0713\n",
      "loss in 1680 batch is 0.5938\n",
      "loss in 1715 batch is 0.0262\n",
      "loss in 1750 batch is 0.2782\n",
      "loss in 1785 batch is 0.1938\n",
      "loss in 1820 batch is 0.0745\n",
      "loss in 1855 batch is 0.4287\n",
      "loss in 1890 batch is 0.1210\n",
      "loss in 1925 batch is 0.2013\n",
      "loss in 1960 batch is 0.0697\n",
      "loss in 1995 batch is 0.1159\n",
      "loss in 2030 batch is 0.5117\n",
      "loss in 2065 batch is 0.0317\n",
      "loss in 2100 batch is 0.4589\n",
      "loss in 2135 batch is 0.0385\n",
      "loss in 2170 batch is 0.2236\n",
      "loss in 2205 batch is 0.1667\n",
      "loss in 2240 batch is 0.3876\n",
      "loss in 2275 batch is 0.0455\n",
      "loss in 2310 batch is 0.1109\n",
      "loss in 2345 batch is 0.0951\n",
      "loss in 2380 batch is 0.0206\n",
      "loss in 2415 batch is 0.2561\n",
      "loss in 2450 batch is 0.0752\n",
      "loss in 2485 batch is 0.1229\n",
      "loss in 2520 batch is 0.1523\n",
      "loss in 2555 batch is 0.0924\n",
      "loss in 2590 batch is 0.1640\n",
      "loss in 2625 batch is 0.2464\n",
      "loss in 2660 batch is 0.2694\n",
      "loss in 2695 batch is 0.0343\n",
      "loss in 2730 batch is 0.6890\n",
      "loss in 2765 batch is 0.9238\n",
      "loss in 2800 batch is 0.3455\n",
      "loss in 2835 batch is 0.0929\n",
      "loss in 2870 batch is 0.0821\n",
      "loss in 2905 batch is 0.1253\n",
      "loss in 2940 batch is 0.0598\n",
      "loss in 2975 batch is 0.0750\n",
      "loss in 3010 batch is 0.3837\n",
      "loss in 3045 batch is 0.1336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in 3080 batch is 0.3316\n",
      "loss in 3115 batch is 0.0801\n",
      "loss in 3150 batch is 0.0837\n",
      "loss in 3185 batch is 0.2358\n",
      "loss in 3220 batch is 0.1733\n",
      "loss in 3255 batch is 0.3165\n",
      "loss in 3290 batch is 0.2255\n",
      "loss in 3325 batch is 0.0970\n",
      "loss in 3360 batch is 0.4588\n",
      "loss in 3395 batch is 0.0620\n",
      "loss in 3430 batch is 0.7692\n",
      "loss in 3465 batch is 0.0789\n",
      "loss in 3500 batch is 0.0295\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.2686 Acc: 0.9010\n",
      "7026\n",
      "loss in 35 batch is 1.1376\n",
      "loss in 70 batch is 0.4889\n",
      "loss in 105 batch is 1.4517\n",
      "loss in 140 batch is 0.1982\n",
      "loss in 175 batch is 2.7226\n",
      "loss in 210 batch is 0.9896\n",
      "loss in 245 batch is 0.2470\n",
      "loss in 280 batch is 1.6799\n",
      "loss in 315 batch is 0.7143\n",
      "loss in 350 batch is 0.8083\n",
      "loss in 385 batch is 0.5226\n",
      "loss in 420 batch is 0.5047\n",
      "loss in 455 batch is 2.8197\n",
      "loss in 490 batch is 0.3080\n",
      "loss in 525 batch is 0.4440\n",
      "loss in 560 batch is 0.8782\n",
      "loss in 595 batch is 1.0066\n",
      "loss in 630 batch is 1.3565\n",
      "loss in 665 batch is 1.9682\n",
      "loss in 700 batch is 1.1440\n",
      "loss in 735 batch is 1.5431\n",
      "loss in 770 batch is 1.4945\n",
      "loss in 805 batch is 0.9858\n",
      "loss in 840 batch is 0.5213\n",
      "loss in 875 batch is 0.8847\n",
      "7026\n",
      "7026\n",
      "val Loss: 1.1962 Acc: 0.7300\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.2543\n",
      "loss in 70 batch is 0.8369\n",
      "loss in 105 batch is 0.1911\n",
      "loss in 140 batch is 0.0507\n",
      "loss in 175 batch is 0.8600\n",
      "loss in 210 batch is 0.1069\n",
      "loss in 245 batch is 0.5735\n",
      "loss in 280 batch is 0.2793\n",
      "loss in 315 batch is 0.2094\n",
      "loss in 350 batch is 0.0849\n",
      "loss in 385 batch is 0.0401\n",
      "loss in 420 batch is 0.0520\n",
      "loss in 455 batch is 0.7000\n",
      "loss in 490 batch is 0.2626\n",
      "loss in 525 batch is 0.3050\n",
      "loss in 560 batch is 0.4206\n",
      "loss in 595 batch is 0.0530\n",
      "loss in 630 batch is 0.1895\n",
      "loss in 665 batch is 0.5480\n",
      "loss in 700 batch is 0.1419\n",
      "loss in 735 batch is 0.1287\n",
      "loss in 770 batch is 0.0167\n",
      "loss in 805 batch is 0.1900\n",
      "loss in 840 batch is 0.0139\n",
      "loss in 875 batch is 0.0775\n",
      "loss in 910 batch is 0.4818\n",
      "loss in 945 batch is 0.1040\n",
      "loss in 980 batch is 0.2050\n",
      "loss in 1015 batch is 0.0915\n",
      "loss in 1050 batch is 0.2236\n",
      "loss in 1085 batch is 0.1230\n",
      "loss in 1120 batch is 0.0682\n",
      "loss in 1155 batch is 0.6842\n",
      "loss in 1190 batch is 0.2742\n",
      "loss in 1225 batch is 0.0659\n",
      "loss in 1260 batch is 0.1074\n",
      "loss in 1295 batch is 0.0898\n",
      "loss in 1330 batch is 0.0818\n",
      "loss in 1365 batch is 0.4444\n",
      "loss in 1400 batch is 0.1436\n",
      "loss in 1435 batch is 0.3112\n",
      "loss in 1470 batch is 0.9685\n",
      "loss in 1505 batch is 0.0683\n",
      "loss in 1540 batch is 0.1150\n",
      "loss in 1575 batch is 0.1957\n",
      "loss in 1610 batch is 0.3127\n",
      "loss in 1645 batch is 0.1469\n",
      "loss in 1680 batch is 0.3758\n",
      "loss in 1715 batch is 0.1751\n",
      "loss in 1750 batch is 0.0549\n",
      "loss in 1785 batch is 0.2877\n",
      "loss in 1820 batch is 0.5247\n",
      "loss in 1855 batch is 0.1087\n",
      "loss in 1890 batch is 0.0677\n",
      "loss in 1925 batch is 0.1588\n",
      "loss in 1960 batch is 0.2774\n",
      "loss in 1995 batch is 0.1874\n",
      "loss in 2030 batch is 0.1545\n",
      "loss in 2065 batch is 0.0774\n",
      "loss in 2100 batch is 0.0972\n",
      "loss in 2135 batch is 0.2430\n",
      "loss in 2170 batch is 0.6206\n",
      "loss in 2205 batch is 0.1891\n",
      "loss in 2240 batch is 0.2404\n",
      "loss in 2275 batch is 0.0979\n",
      "loss in 2310 batch is 0.4942\n",
      "loss in 2345 batch is 0.1659\n",
      "loss in 2380 batch is 0.1876\n",
      "loss in 2415 batch is 0.0712\n",
      "loss in 2450 batch is 0.5590\n",
      "loss in 2485 batch is 0.1950\n",
      "loss in 2520 batch is 0.0677\n",
      "loss in 2555 batch is 0.0366\n",
      "loss in 2590 batch is 0.2744\n",
      "loss in 2625 batch is 0.7642\n",
      "loss in 2660 batch is 0.3272\n",
      "loss in 2695 batch is 0.0781\n",
      "loss in 2730 batch is 0.3454\n",
      "loss in 2765 batch is 0.2781\n",
      "loss in 2800 batch is 0.0205\n",
      "loss in 2835 batch is 0.4264\n",
      "loss in 2870 batch is 0.3327\n",
      "loss in 2905 batch is 0.0703\n",
      "loss in 2940 batch is 0.1019\n",
      "loss in 2975 batch is 0.5331\n",
      "loss in 3010 batch is 0.4314\n",
      "loss in 3045 batch is 0.0431\n",
      "loss in 3080 batch is 0.2804\n",
      "loss in 3115 batch is 0.0720\n",
      "loss in 3150 batch is 0.1150\n",
      "loss in 3185 batch is 0.2341\n",
      "loss in 3220 batch is 0.1543\n",
      "loss in 3255 batch is 0.0307\n",
      "loss in 3290 batch is 0.2365\n",
      "loss in 3325 batch is 0.0797\n",
      "loss in 3360 batch is 0.1636\n",
      "loss in 3395 batch is 0.1286\n",
      "loss in 3430 batch is 0.8175\n",
      "loss in 3465 batch is 0.1462\n",
      "loss in 3500 batch is 0.0119\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.2330 Acc: 0.9123\n",
      "7026\n",
      "loss in 35 batch is 0.7450\n",
      "loss in 70 batch is 0.1347\n",
      "loss in 105 batch is 0.5237\n",
      "loss in 140 batch is 0.0845\n",
      "loss in 175 batch is 2.1435\n",
      "loss in 210 batch is 1.5901\n",
      "loss in 245 batch is 0.2398\n",
      "loss in 280 batch is 0.6727\n",
      "loss in 315 batch is 0.4082\n",
      "loss in 350 batch is 0.5783\n",
      "loss in 385 batch is 1.1240\n",
      "loss in 420 batch is 0.8509\n",
      "loss in 455 batch is 1.8029\n",
      "loss in 490 batch is 0.3332\n",
      "loss in 525 batch is 1.4287\n",
      "loss in 560 batch is 0.6166\n",
      "loss in 595 batch is 1.0400\n",
      "loss in 630 batch is 2.8317\n",
      "loss in 665 batch is 2.3879\n",
      "loss in 700 batch is 0.4275\n",
      "loss in 735 batch is 0.8544\n",
      "loss in 770 batch is 1.8512\n",
      "loss in 805 batch is 0.1995\n",
      "loss in 840 batch is 0.4307\n",
      "loss in 875 batch is 0.6846\n",
      "7026\n",
      "7026\n",
      "val Loss: 1.2178 Acc: 0.7210\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.0048\n",
      "loss in 70 batch is 0.1520\n",
      "loss in 105 batch is 0.0284\n",
      "loss in 140 batch is 0.1275\n",
      "loss in 175 batch is 0.4101\n",
      "loss in 210 batch is 0.0144\n",
      "loss in 245 batch is 0.6830\n",
      "loss in 280 batch is 0.1787\n",
      "loss in 315 batch is 0.0810\n",
      "loss in 350 batch is 0.3121\n",
      "loss in 385 batch is 0.0160\n",
      "loss in 420 batch is 0.0128\n",
      "loss in 455 batch is 0.4425\n",
      "loss in 490 batch is 0.4427\n",
      "loss in 525 batch is 0.0271\n",
      "loss in 560 batch is 0.0765\n",
      "loss in 595 batch is 0.0939\n",
      "loss in 630 batch is 0.0220\n",
      "loss in 665 batch is 0.4171\n",
      "loss in 700 batch is 0.2106\n",
      "loss in 735 batch is 0.2834\n",
      "loss in 770 batch is 0.0298\n",
      "loss in 805 batch is 0.0160\n",
      "loss in 840 batch is 0.0738\n",
      "loss in 875 batch is 0.0398\n",
      "loss in 910 batch is 0.0727\n",
      "loss in 945 batch is 0.2931\n",
      "loss in 980 batch is 0.0857\n",
      "loss in 1015 batch is 0.0666\n",
      "loss in 1050 batch is 0.4241\n",
      "loss in 1085 batch is 0.0471\n",
      "loss in 1120 batch is 0.1269\n",
      "loss in 1155 batch is 0.3616\n",
      "loss in 1190 batch is 0.0343\n",
      "loss in 1225 batch is 0.5851\n",
      "loss in 1260 batch is 0.0368\n",
      "loss in 1295 batch is 0.2196\n",
      "loss in 1330 batch is 0.0125\n",
      "loss in 1365 batch is 0.0630\n",
      "loss in 1400 batch is 0.1787\n",
      "loss in 1435 batch is 0.4861\n",
      "loss in 1470 batch is 0.4041\n",
      "loss in 1505 batch is 0.1306\n",
      "loss in 1540 batch is 0.0557\n",
      "loss in 1575 batch is 0.2552\n",
      "loss in 1610 batch is 0.0637\n",
      "loss in 1645 batch is 0.0069\n",
      "loss in 1680 batch is 0.0659\n",
      "loss in 1715 batch is 0.1518\n",
      "loss in 1750 batch is 0.1228\n",
      "loss in 1785 batch is 0.0805\n",
      "loss in 1820 batch is 0.1140\n",
      "loss in 1855 batch is 0.2836\n",
      "loss in 1890 batch is 0.2939\n",
      "loss in 1925 batch is 0.0364\n",
      "loss in 1960 batch is 0.4200\n",
      "loss in 1995 batch is 0.3782\n",
      "loss in 2030 batch is 0.3292\n",
      "loss in 2065 batch is 0.0389\n",
      "loss in 2100 batch is 0.3174\n",
      "loss in 2135 batch is 0.0848\n",
      "loss in 2170 batch is 0.0221\n",
      "loss in 2205 batch is 0.0334\n",
      "loss in 2240 batch is 0.0114\n",
      "loss in 2275 batch is 0.0709\n",
      "loss in 2310 batch is 0.0194\n",
      "loss in 2345 batch is 0.0860\n",
      "loss in 2380 batch is 0.1392\n",
      "loss in 2415 batch is 0.4045\n",
      "loss in 2450 batch is 0.2107\n",
      "loss in 2485 batch is 0.2910\n",
      "loss in 2520 batch is 0.1592\n",
      "loss in 2555 batch is 0.0414\n",
      "loss in 2590 batch is 0.4020\n",
      "loss in 2625 batch is 0.0355\n",
      "loss in 2660 batch is 0.0767\n",
      "loss in 2695 batch is 0.2038\n",
      "loss in 2730 batch is 0.2556\n",
      "loss in 2765 batch is 0.0970\n",
      "loss in 2800 batch is 0.0640\n",
      "loss in 2835 batch is 0.0915\n",
      "loss in 2870 batch is 0.0574\n",
      "loss in 2905 batch is 0.0663\n",
      "loss in 2940 batch is 0.3132\n",
      "loss in 2975 batch is 0.3487\n",
      "loss in 3010 batch is 0.5161\n",
      "loss in 3045 batch is 0.0081\n",
      "loss in 3080 batch is 0.4308\n",
      "loss in 3115 batch is 0.0767\n",
      "loss in 3150 batch is 0.0564\n",
      "loss in 3185 batch is 0.0976\n",
      "loss in 3220 batch is 0.4582\n",
      "loss in 3255 batch is 0.7540\n",
      "loss in 3290 batch is 0.0532\n",
      "loss in 3325 batch is 0.0146\n",
      "loss in 3360 batch is 0.0345\n",
      "loss in 3395 batch is 0.0559\n",
      "loss in 3430 batch is 0.6935\n",
      "loss in 3465 batch is 0.0657\n",
      "loss in 3500 batch is 0.0735\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.1919 Acc: 0.9285\n",
      "7026\n",
      "loss in 35 batch is 1.0779\n",
      "loss in 70 batch is 0.6931\n",
      "loss in 105 batch is 0.9790\n",
      "loss in 140 batch is 0.1352\n",
      "loss in 175 batch is 2.5554\n",
      "loss in 210 batch is 0.9477\n",
      "loss in 245 batch is 0.3672\n",
      "loss in 280 batch is 0.3882\n",
      "loss in 315 batch is 0.4354\n",
      "loss in 350 batch is 0.5441\n",
      "loss in 385 batch is 1.5047\n",
      "loss in 420 batch is 1.0538\n",
      "loss in 455 batch is 1.9860\n",
      "loss in 490 batch is 0.5315\n",
      "loss in 525 batch is 1.0229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in 560 batch is 0.2956\n",
      "loss in 595 batch is 3.0265\n",
      "loss in 630 batch is 1.0199\n",
      "loss in 665 batch is 2.2584\n",
      "loss in 700 batch is 1.8161\n",
      "loss in 735 batch is 0.9925\n",
      "loss in 770 batch is 2.3630\n",
      "loss in 805 batch is 0.7308\n",
      "loss in 840 batch is 0.3576\n",
      "loss in 875 batch is 0.0839\n",
      "7026\n",
      "7026\n",
      "val Loss: 1.3944 Acc: 0.7348\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "28100\n",
      "loss in 35 batch is 0.5678\n",
      "loss in 70 batch is 0.1312\n",
      "loss in 105 batch is 0.6469\n",
      "loss in 140 batch is 0.1249\n",
      "loss in 175 batch is 0.1347\n",
      "loss in 210 batch is 0.1496\n",
      "loss in 245 batch is 0.3665\n",
      "loss in 280 batch is 0.0128\n",
      "loss in 315 batch is 0.1152\n",
      "loss in 350 batch is 0.0215\n",
      "loss in 385 batch is 0.0065\n",
      "loss in 420 batch is 0.1495\n",
      "loss in 455 batch is 0.0490\n",
      "loss in 490 batch is 0.0206\n",
      "loss in 525 batch is 0.0258\n",
      "loss in 560 batch is 0.3046\n",
      "loss in 595 batch is 0.0212\n",
      "loss in 630 batch is 0.2952\n",
      "loss in 665 batch is 0.2606\n",
      "loss in 700 batch is 0.1636\n",
      "loss in 735 batch is 0.2132\n",
      "loss in 770 batch is 0.0215\n",
      "loss in 805 batch is 0.0242\n",
      "loss in 840 batch is 0.1076\n",
      "loss in 875 batch is 0.3100\n",
      "loss in 910 batch is 0.1246\n",
      "loss in 945 batch is 0.0802\n",
      "loss in 980 batch is 0.1939\n",
      "loss in 1015 batch is 0.2911\n",
      "loss in 1050 batch is 0.0324\n",
      "loss in 1085 batch is 0.3390\n",
      "loss in 1120 batch is 0.1162\n",
      "loss in 1155 batch is 0.2184\n",
      "loss in 1190 batch is 0.0663\n",
      "loss in 1225 batch is 0.0287\n",
      "loss in 1260 batch is 0.0667\n",
      "loss in 1295 batch is 0.3304\n",
      "loss in 1330 batch is 0.0166\n",
      "loss in 1365 batch is 0.1292\n",
      "loss in 1400 batch is 0.0301\n",
      "loss in 1435 batch is 0.4045\n",
      "loss in 1470 batch is 0.0812\n",
      "loss in 1505 batch is 0.3184\n",
      "loss in 1540 batch is 0.1479\n",
      "loss in 1575 batch is 0.0119\n",
      "loss in 1610 batch is 0.0775\n",
      "loss in 1645 batch is 0.0634\n",
      "loss in 1680 batch is 0.0269\n",
      "loss in 1715 batch is 0.0534\n",
      "loss in 1750 batch is 0.2510\n",
      "loss in 1785 batch is 0.1411\n",
      "loss in 1820 batch is 0.0742\n",
      "loss in 1855 batch is 0.1363\n",
      "loss in 1890 batch is 0.0477\n",
      "loss in 1925 batch is 0.0126\n",
      "loss in 1960 batch is 0.1307\n",
      "loss in 1995 batch is 0.1553\n",
      "loss in 2030 batch is 0.1872\n",
      "loss in 2065 batch is 0.0345\n",
      "loss in 2100 batch is 0.0172\n",
      "loss in 2135 batch is 0.0098\n",
      "loss in 2170 batch is 0.1410\n",
      "loss in 2205 batch is 0.0209\n",
      "loss in 2240 batch is 0.0108\n",
      "loss in 2275 batch is 0.3223\n",
      "loss in 2310 batch is 0.0329\n",
      "loss in 2345 batch is 0.0375\n",
      "loss in 2380 batch is 0.0816\n",
      "loss in 2415 batch is 0.4104\n",
      "loss in 2450 batch is 0.0993\n",
      "loss in 2485 batch is 0.2515\n",
      "loss in 2520 batch is 0.0223\n",
      "loss in 2555 batch is 0.0220\n",
      "loss in 2590 batch is 0.4804\n",
      "loss in 2625 batch is 0.2054\n",
      "loss in 2660 batch is 0.3654\n",
      "loss in 2695 batch is 0.0114\n",
      "loss in 2730 batch is 0.3324\n",
      "loss in 2765 batch is 1.0486\n",
      "loss in 2800 batch is 0.0280\n",
      "loss in 2835 batch is 0.0472\n",
      "loss in 2870 batch is 0.0432\n",
      "loss in 2905 batch is 0.0605\n",
      "loss in 2940 batch is 0.0178\n",
      "loss in 2975 batch is 0.1921\n",
      "loss in 3010 batch is 0.1489\n",
      "loss in 3045 batch is 0.0093\n",
      "loss in 3080 batch is 0.0584\n",
      "loss in 3115 batch is 0.0778\n",
      "loss in 3150 batch is 0.0809\n",
      "loss in 3185 batch is 0.0356\n",
      "loss in 3220 batch is 0.1463\n",
      "loss in 3255 batch is 0.0146\n",
      "loss in 3290 batch is 0.0485\n",
      "loss in 3325 batch is 0.3960\n",
      "loss in 3360 batch is 0.1101\n",
      "loss in 3395 batch is 0.0561\n",
      "loss in 3430 batch is 0.8102\n",
      "loss in 3465 batch is 0.0398\n",
      "loss in 3500 batch is 0.0076\n",
      "28100\n",
      "28100\n",
      "train Loss: 0.1649 Acc: 0.9400\n",
      "7026\n",
      "loss in 35 batch is 1.8157\n",
      "loss in 70 batch is 0.6352\n",
      "loss in 105 batch is 3.7343\n",
      "loss in 140 batch is 0.0223\n",
      "loss in 175 batch is 3.1646\n",
      "loss in 210 batch is 0.9507\n",
      "loss in 245 batch is 0.7174\n",
      "loss in 280 batch is 0.9496\n",
      "loss in 315 batch is 0.2841\n",
      "loss in 350 batch is 0.4963\n",
      "loss in 385 batch is 0.3392\n",
      "loss in 420 batch is 0.8817\n",
      "loss in 455 batch is 3.8553\n",
      "loss in 490 batch is 0.4088\n",
      "loss in 525 batch is 3.6725\n",
      "loss in 560 batch is 0.0148\n",
      "loss in 595 batch is 2.6617\n",
      "loss in 630 batch is 3.7499\n",
      "loss in 665 batch is 3.4803\n",
      "loss in 700 batch is 1.5595\n",
      "loss in 735 batch is 1.5438\n",
      "loss in 770 batch is 2.4276\n",
      "loss in 805 batch is 1.0512\n",
      "loss in 840 batch is 1.8901\n",
      "loss in 875 batch is 0.4822\n",
      "7026\n",
      "7026\n",
      "val Loss: 1.8272 Acc: 0.7647\n",
      "\n",
      "Training complete in 86m 56s\n",
      "Best val Acc: 0.781099\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,\n",
    "                             \"resnet50_12epoch.checkpoint\", num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "# model_toy, hist = train_model(model_toy, dataloaders_dict, criterion, optimizer_ft, \n",
    "#                               checkpoint,num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#             'epoch': 1,\n",
    "#             'model_state_dict': model_toy.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "#             }, './vgg19toy.checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft,'resnet50_12epoch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(idx):\n",
    "        imgtemp = Image.open(images_train_list[idx])\n",
    "#         if self.transform!=None :\n",
    "#             imgtemp = self.transform(imgtemp)\n",
    "        img = transforms.ToTensor()(imgtemp)\n",
    "        fileName = images_train_list[idx].split('/')[-1].split('.')[0]\n",
    "        imglabel = torch.zeros(5,dtype=torch.long)\n",
    "        imglabel[trainLabels.loc[trainLabels.image==fileName ,'level'].values[0]] = 1\n",
    "        return img ,imglabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_for_one_image(idx,model):\n",
    "        test , test_label = get_input(idx)\n",
    "        test = test.view(-1,3,224,224)\n",
    "        test_label = test_label.view(-1,5)\n",
    "        test = test.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "        print(\"test_label\",test_label)\n",
    "        pred = model(test)\n",
    "        print(\"pred\",pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx  0\n",
      "idx  1000\n",
      "idx  2000\n",
      "idx  3000\n",
      "idx  4000\n",
      "idx  5000\n",
      "idx  6000\n",
      "idx  7000\n",
      "idx  8000\n",
      "idx  9000\n",
      "idx  10000\n",
      "idx  11000\n",
      "idx  12000\n",
      "idx  13000\n",
      "idx  14000\n",
      "idx  15000\n",
      "idx  16000\n",
      "idx  17000\n",
      "idx  18000\n",
      "idx  19000\n",
      "idx  20000\n",
      "idx  21000\n",
      "idx  22000\n",
      "idx  23000\n",
      "idx  24000\n",
      "idx  25000\n",
      "idx  26000\n",
      "idx  27000\n",
      "idx  28000\n"
     ]
    }
   ],
   "source": [
    "class_indices = {0: [] , 1: [] , 2: [] , 3 : [], 4:[]}\n",
    "for idx in range(len(images_train_list)):\n",
    "    fileName = images_train_list[idx].split('/')[-1].split('.')[0]\n",
    "    label = torch.zeros(5,dtype=torch.long)\n",
    "    label[trainLabels.loc[trainLabels.image==fileName ,'level'].values[0]] = 1\n",
    "#     print(label)\n",
    "    _, img_tru_class = torch.max(label,0)\n",
    "#     print(img_tru_class)\n",
    "    if idx%1000==0:\n",
    "        print(\"idx \",idx)\n",
    "    class_indices[img_tru_class.item()].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_index(idx,img_list):\n",
    "    img = Image.open(img_list[idx])\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train/train_224/5789_left.tiff'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train_list[class_indices[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 3\n",
      "test_label tensor([[0, 0, 1, 0, 0]], device='cuda:0')\n",
      "pred tensor([[ 1.3853, -0.2005,  2.1212, -1.4302, -1.9960]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "index 4\n",
      "test_label tensor([[0, 0, 1, 0, 0]], device='cuda:0')\n",
      "pred tensor([[ 2.5631,  1.1130,  0.6820, -2.8097, -1.7097]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "index 10\n",
      "test_label tensor([[0, 0, 1, 0, 0]], device='cuda:0')\n",
      "pred tensor([[ 2.0091,  0.5335,  1.1184, -2.6434, -1.2409]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "index 15\n",
      "test_label tensor([[0, 0, 1, 0, 0]], device='cuda:0')\n",
      "pred tensor([[ 1.7963,  0.2341,  3.2350, -1.7537, -3.4278]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "index 24\n",
      "test_label tensor([[0, 0, 1, 0, 0]], device='cuda:0')\n",
      "pred tensor([[-5.3421, -4.9266,  4.3555,  3.5650,  2.3408]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    index = class_indices[2][i]\n",
    "    print(\"index\",index)\n",
    "    get_pred_for_one_image(class_indices[2][i],model_ft)\n",
    "    visualize_index(class_indices[2][i],images_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_index(24,images_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train/train_224/35193_left.tiff',\n",
       " './train/train_224/23392_left.tiff',\n",
       " './train/train_224/2199_left.tiff',\n",
       " './train/train_224/2536_left.tiff',\n",
       " './train/train_224/43589_right.tiff',\n",
       " './train/train_224/3341_right.tiff',\n",
       " './train/train_224/23251_left.tiff',\n",
       " './train/train_224/20863_right.tiff',\n",
       " './train/train_224/10484_right.tiff',\n",
       " './train/train_224/5827_right.tiff',\n",
       " './train/train_224/3301_left.tiff',\n",
       " './train/train_224/6541_left.tiff',\n",
       " './train/train_224/13062_right.tiff',\n",
       " './train/train_224/27635_left.tiff',\n",
       " './train/train_224/10294_right.tiff',\n",
       " './train/train_224/36636_left.tiff',\n",
       " './train/train_224/19177_right.tiff',\n",
       " './train/train_224/2853_right.tiff',\n",
       " './train/train_224/7174_right.tiff',\n",
       " './train/train_224/3908_left.tiff',\n",
       " './train/train_224/19867_right.tiff',\n",
       " './train/train_224/9346_right.tiff',\n",
       " './train/train_224/22134_left.tiff',\n",
       " './train/train_224/5067_left.tiff',\n",
       " './train/train_224/1965_right.tiff',\n",
       " './train/train_224/35464_left.tiff',\n",
       " './train/train_224/8888_right.tiff',\n",
       " './train/train_224/10278_right.tiff',\n",
       " './train/train_224/14398_left.tiff',\n",
       " './train/train_224/16620_left.tiff',\n",
       " './train/train_224/31751_left.tiff',\n",
       " './train/train_224/40186_left.tiff',\n",
       " './train/train_224/482_right.tiff',\n",
       " './train/train_224/5789_left.tiff',\n",
       " './train/train_224/14093_right.tiff',\n",
       " './train/train_224/39791_right.tiff',\n",
       " './train/train_224/10904_left.tiff',\n",
       " './train/train_224/12523_left.tiff',\n",
       " './train/train_224/3639_left.tiff',\n",
       " './train/train_224/32401_right.tiff',\n",
       " './train/train_224/26131_left.tiff',\n",
       " './train/train_224/38583_right.tiff',\n",
       " './train/train_224/34902_right.tiff',\n",
       " './train/train_224/9384_left.tiff',\n",
       " './train/train_224/5508_left.tiff',\n",
       " './train/train_224/23665_left.tiff',\n",
       " './train/train_224/9099_left.tiff',\n",
       " './train/train_224/34541_left.tiff',\n",
       " './train/train_224/4615_left.tiff',\n",
       " './train/train_224/41847_left.tiff',\n",
       " './train/train_224/15023_right.tiff',\n",
       " './train/train_224/23970_right.tiff',\n",
       " './train/train_224/28721_left.tiff',\n",
       " './train/train_224/27727_left.tiff',\n",
       " './train/train_224/449_left.tiff',\n",
       " './train/train_224/30573_right.tiff',\n",
       " './train/train_224/23182_right.tiff',\n",
       " './train/train_224/42922_right.tiff',\n",
       " './train/train_224/32148_left.tiff',\n",
       " './train/train_224/14152_right.tiff',\n",
       " './train/train_224/21023_left.tiff',\n",
       " './train/train_224/32098_left.tiff',\n",
       " './train/train_224/3660_right.tiff',\n",
       " './train/train_224/28799_right.tiff',\n",
       " './train/train_224/3416_right.tiff',\n",
       " './train/train_224/5209_right.tiff',\n",
       " './train/train_224/5349_right.tiff',\n",
       " './train/train_224/11980_right.tiff',\n",
       " './train/train_224/2668_left.tiff',\n",
       " './train/train_224/10614_right.tiff',\n",
       " './train/train_224/25992_right.tiff',\n",
       " './train/train_224/39272_right.tiff',\n",
       " './train/train_224/37087_left.tiff',\n",
       " './train/train_224/31704_right.tiff',\n",
       " './train/train_224/8202_left.tiff',\n",
       " './train/train_224/3000_left.tiff',\n",
       " './train/train_224/26240_left.tiff',\n",
       " './train/train_224/17366_left.tiff',\n",
       " './train/train_224/14028_left.tiff',\n",
       " './train/train_224/44254_right.tiff',\n",
       " './train/train_224/39322_right.tiff',\n",
       " './train/train_224/14530_right.tiff',\n",
       " './train/train_224/947_left.tiff',\n",
       " './train/train_224/8874_left.tiff',\n",
       " './train/train_224/11198_left.tiff',\n",
       " './train/train_224/36179_left.tiff',\n",
       " './train/train_224/22854_left.tiff',\n",
       " './train/train_224/7727_right.tiff',\n",
       " './train/train_224/23916_right.tiff',\n",
       " './train/train_224/21077_right.tiff',\n",
       " './train/train_224/21826_right.tiff',\n",
       " './train/train_224/9168_left.tiff',\n",
       " './train/train_224/33272_left.tiff',\n",
       " './train/train_224/26530_right.tiff',\n",
       " './train/train_224/15045_left.tiff',\n",
       " './train/train_224/19160_left.tiff',\n",
       " './train/train_224/26874_right.tiff',\n",
       " './train/train_224/17057_right.tiff',\n",
       " './train/train_224/27035_right.tiff',\n",
       " './train/train_224/8407_right.tiff',\n",
       " './train/train_224/6443_left.tiff',\n",
       " './train/train_224/30043_left.tiff',\n",
       " './train/train_224/5692_left.tiff',\n",
       " './train/train_224/15375_left.tiff',\n",
       " './train/train_224/19614_right.tiff',\n",
       " './train/train_224/26426_left.tiff',\n",
       " './train/train_224/22825_left.tiff',\n",
       " './train/train_224/13773_left.tiff',\n",
       " './train/train_224/17169_right.tiff',\n",
       " './train/train_224/9646_right.tiff',\n",
       " './train/train_224/34995_left.tiff',\n",
       " './train/train_224/26921_right.tiff',\n",
       " './train/train_224/9844_left.tiff',\n",
       " './train/train_224/41419_left.tiff',\n",
       " './train/train_224/16540_left.tiff',\n",
       " './train/train_224/1981_right.tiff',\n",
       " './train/train_224/15477_left.tiff',\n",
       " './train/train_224/25005_left.tiff',\n",
       " './train/train_224/12370_right.tiff',\n",
       " './train/train_224/14175_left.tiff',\n",
       " './train/train_224/26309_right.tiff',\n",
       " './train/train_224/3865_right.tiff',\n",
       " './train/train_224/24996_right.tiff',\n",
       " './train/train_224/12700_right.tiff',\n",
       " './train/train_224/36919_left.tiff',\n",
       " './train/train_224/4824_left.tiff',\n",
       " './train/train_224/18178_right.tiff',\n",
       " './train/train_224/27179_left.tiff',\n",
       " './train/train_224/38661_left.tiff',\n",
       " './train/train_224/17957_left.tiff',\n",
       " './train/train_224/18597_right.tiff',\n",
       " './train/train_224/36362_left.tiff',\n",
       " './train/train_224/32035_right.tiff',\n",
       " './train/train_224/33163_left.tiff',\n",
       " './train/train_224/9603_right.tiff',\n",
       " './train/train_224/39321_left.tiff',\n",
       " './train/train_224/7613_left.tiff',\n",
       " './train/train_224/5763_right.tiff',\n",
       " './train/train_224/3700_right.tiff',\n",
       " './train/train_224/11726_left.tiff',\n",
       " './train/train_224/43305_left.tiff',\n",
       " './train/train_224/21377_right.tiff',\n",
       " './train/train_224/13645_left.tiff',\n",
       " './train/train_224/3897_right.tiff',\n",
       " './train/train_224/32500_right.tiff',\n",
       " './train/train_224/36376_left.tiff',\n",
       " './train/train_224/29043_left.tiff',\n",
       " './train/train_224/11504_right.tiff',\n",
       " './train/train_224/31226_right.tiff',\n",
       " './train/train_224/39914_left.tiff',\n",
       " './train/train_224/28898_left.tiff',\n",
       " './train/train_224/32112_right.tiff',\n",
       " './train/train_224/566_left.tiff',\n",
       " './train/train_224/28246_right.tiff',\n",
       " './train/train_224/31766_right.tiff',\n",
       " './train/train_224/39477_left.tiff',\n",
       " './train/train_224/20407_right.tiff',\n",
       " './train/train_224/31994_left.tiff',\n",
       " './train/train_224/20777_left.tiff',\n",
       " './train/train_224/8486_left.tiff',\n",
       " './train/train_224/20054_right.tiff',\n",
       " './train/train_224/18060_right.tiff',\n",
       " './train/train_224/35046_left.tiff',\n",
       " './train/train_224/3650_left.tiff',\n",
       " './train/train_224/2903_right.tiff',\n",
       " './train/train_224/30970_left.tiff',\n",
       " './train/train_224/445_left.tiff',\n",
       " './train/train_224/23123_right.tiff',\n",
       " './train/train_224/10389_left.tiff',\n",
       " './train/train_224/7700_left.tiff',\n",
       " './train/train_224/13969_left.tiff',\n",
       " './train/train_224/39841_right.tiff',\n",
       " './train/train_224/34072_left.tiff',\n",
       " './train/train_224/23326_left.tiff',\n",
       " './train/train_224/36628_left.tiff',\n",
       " './train/train_224/16866_right.tiff',\n",
       " './train/train_224/10672_left.tiff',\n",
       " './train/train_224/9889_left.tiff',\n",
       " './train/train_224/11164_right.tiff',\n",
       " './train/train_224/22365_right.tiff',\n",
       " './train/train_224/32642_left.tiff',\n",
       " './train/train_224/5990_right.tiff',\n",
       " './train/train_224/14902_left.tiff',\n",
       " './train/train_224/22561_right.tiff',\n",
       " './train/train_224/14046_right.tiff',\n",
       " './train/train_224/13935_right.tiff',\n",
       " './train/train_224/24727_right.tiff',\n",
       " './train/train_224/8897_left.tiff',\n",
       " './train/train_224/37974_right.tiff',\n",
       " './train/train_224/4071_left.tiff',\n",
       " './train/train_224/19070_right.tiff',\n",
       " './train/train_224/7361_left.tiff',\n",
       " './train/train_224/34655_right.tiff',\n",
       " './train/train_224/1452_left.tiff',\n",
       " './train/train_224/7390_left.tiff',\n",
       " './train/train_224/9120_right.tiff',\n",
       " './train/train_224/33325_right.tiff',\n",
       " './train/train_224/7410_left.tiff',\n",
       " './train/train_224/4518_left.tiff',\n",
       " './train/train_224/24645_right.tiff',\n",
       " './train/train_224/13453_left.tiff',\n",
       " './train/train_224/34641_left.tiff',\n",
       " './train/train_224/10250_left.tiff',\n",
       " './train/train_224/8827_left.tiff',\n",
       " './train/train_224/25074_right.tiff',\n",
       " './train/train_224/2058_left.tiff',\n",
       " './train/train_224/18576_right.tiff',\n",
       " './train/train_224/6840_right.tiff',\n",
       " './train/train_224/5490_left.tiff',\n",
       " './train/train_224/41641_left.tiff',\n",
       " './train/train_224/36288_right.tiff',\n",
       " './train/train_224/20022_left.tiff',\n",
       " './train/train_224/44079_right.tiff',\n",
       " './train/train_224/27374_right.tiff',\n",
       " './train/train_224/28758_right.tiff',\n",
       " './train/train_224/30967_left.tiff',\n",
       " './train/train_224/28273_left.tiff',\n",
       " './train/train_224/33615_left.tiff',\n",
       " './train/train_224/2217_left.tiff',\n",
       " './train/train_224/10657_right.tiff',\n",
       " './train/train_224/15481_left.tiff',\n",
       " './train/train_224/16131_left.tiff',\n",
       " './train/train_224/29472_left.tiff',\n",
       " './train/train_224/3444_right.tiff',\n",
       " './train/train_224/36572_left.tiff',\n",
       " './train/train_224/26464_left.tiff',\n",
       " './train/train_224/8236_right.tiff',\n",
       " './train/train_224/7190_left.tiff',\n",
       " './train/train_224/12971_right.tiff',\n",
       " './train/train_224/3608_right.tiff',\n",
       " './train/train_224/29858_right.tiff',\n",
       " './train/train_224/3865_left.tiff',\n",
       " './train/train_224/28076_right.tiff',\n",
       " './train/train_224/27299_right.tiff',\n",
       " './train/train_224/36427_left.tiff',\n",
       " './train/train_224/2351_left.tiff',\n",
       " './train/train_224/28913_right.tiff',\n",
       " './train/train_224/22979_left.tiff',\n",
       " './train/train_224/19813_left.tiff',\n",
       " './train/train_224/343_left.tiff',\n",
       " './train/train_224/12575_left.tiff',\n",
       " './train/train_224/17311_left.tiff',\n",
       " './train/train_224/21179_left.tiff',\n",
       " './train/train_224/17098_right.tiff',\n",
       " './train/train_224/8395_left.tiff',\n",
       " './train/train_224/29423_left.tiff',\n",
       " './train/train_224/473_right.tiff',\n",
       " './train/train_224/6594_left.tiff',\n",
       " './train/train_224/2275_left.tiff',\n",
       " './train/train_224/33858_left.tiff',\n",
       " './train/train_224/42784_left.tiff',\n",
       " './train/train_224/29423_right.tiff',\n",
       " './train/train_224/43773_left.tiff',\n",
       " './train/train_224/43988_left.tiff',\n",
       " './train/train_224/3040_right.tiff',\n",
       " './train/train_224/40119_right.tiff',\n",
       " './train/train_224/36002_right.tiff',\n",
       " './train/train_224/10961_left.tiff',\n",
       " './train/train_224/40442_left.tiff',\n",
       " './train/train_224/9324_left.tiff',\n",
       " './train/train_224/1021_left.tiff',\n",
       " './train/train_224/6412_left.tiff',\n",
       " './train/train_224/15638_right.tiff',\n",
       " './train/train_224/2605_right.tiff',\n",
       " './train/train_224/12433_right.tiff',\n",
       " './train/train_224/34186_right.tiff',\n",
       " './train/train_224/14863_left.tiff',\n",
       " './train/train_224/37493_right.tiff',\n",
       " './train/train_224/7522_right.tiff',\n",
       " './train/train_224/16027_right.tiff',\n",
       " './train/train_224/42358_right.tiff',\n",
       " './train/train_224/38136_right.tiff',\n",
       " './train/train_224/2823_right.tiff',\n",
       " './train/train_224/15089_right.tiff',\n",
       " './train/train_224/12714_left.tiff',\n",
       " './train/train_224/37951_left.tiff',\n",
       " './train/train_224/26830_right.tiff',\n",
       " './train/train_224/31710_left.tiff',\n",
       " './train/train_224/35931_left.tiff',\n",
       " './train/train_224/3335_left.tiff',\n",
       " './train/train_224/31439_left.tiff',\n",
       " './train/train_224/31098_left.tiff',\n",
       " './train/train_224/43359_right.tiff',\n",
       " './train/train_224/26243_left.tiff',\n",
       " './train/train_224/24737_right.tiff',\n",
       " './train/train_224/22822_left.tiff',\n",
       " './train/train_224/39970_right.tiff',\n",
       " './train/train_224/36500_right.tiff',\n",
       " './train/train_224/18105_left.tiff',\n",
       " './train/train_224/18298_left.tiff',\n",
       " './train/train_224/38118_right.tiff',\n",
       " './train/train_224/43814_right.tiff',\n",
       " './train/train_224/9826_right.tiff',\n",
       " './train/train_224/12654_left.tiff',\n",
       " './train/train_224/21090_right.tiff',\n",
       " './train/train_224/9402_right.tiff',\n",
       " './train/train_224/24410_left.tiff',\n",
       " './train/train_224/9288_left.tiff',\n",
       " './train/train_224/19553_left.tiff',\n",
       " './train/train_224/41271_left.tiff',\n",
       " './train/train_224/11098_right.tiff',\n",
       " './train/train_224/20455_left.tiff',\n",
       " './train/train_224/27539_left.tiff',\n",
       " './train/train_224/32180_right.tiff',\n",
       " './train/train_224/24611_left.tiff',\n",
       " './train/train_224/42553_right.tiff',\n",
       " './train/train_224/3348_left.tiff',\n",
       " './train/train_224/17052_left.tiff',\n",
       " './train/train_224/24472_left.tiff',\n",
       " './train/train_224/21238_right.tiff',\n",
       " './train/train_224/2962_right.tiff',\n",
       " './train/train_224/32024_left.tiff',\n",
       " './train/train_224/18422_right.tiff',\n",
       " './train/train_224/9614_left.tiff',\n",
       " './train/train_224/25141_left.tiff',\n",
       " './train/train_224/42377_right.tiff',\n",
       " './train/train_224/36745_right.tiff',\n",
       " './train/train_224/36141_left.tiff',\n",
       " './train/train_224/19269_left.tiff',\n",
       " './train/train_224/44062_left.tiff',\n",
       " './train/train_224/36671_right.tiff',\n",
       " './train/train_224/30910_right.tiff',\n",
       " './train/train_224/2872_left.tiff',\n",
       " './train/train_224/6965_right.tiff',\n",
       " './train/train_224/18696_left.tiff',\n",
       " './train/train_224/30833_left.tiff',\n",
       " './train/train_224/19840_right.tiff',\n",
       " './train/train_224/12595_right.tiff',\n",
       " './train/train_224/19486_left.tiff',\n",
       " './train/train_224/41349_right.tiff',\n",
       " './train/train_224/33975_left.tiff',\n",
       " './train/train_224/10792_left.tiff',\n",
       " './train/train_224/25770_left.tiff',\n",
       " './train/train_224/34651_right.tiff',\n",
       " './train/train_224/39644_right.tiff',\n",
       " './train/train_224/15003_right.tiff',\n",
       " './train/train_224/26879_left.tiff',\n",
       " './train/train_224/28629_right.tiff',\n",
       " './train/train_224/31669_left.tiff',\n",
       " './train/train_224/3657_left.tiff',\n",
       " './train/train_224/41573_left.tiff',\n",
       " './train/train_224/35421_left.tiff',\n",
       " './train/train_224/2597_right.tiff',\n",
       " './train/train_224/22048_left.tiff',\n",
       " './train/train_224/14898_left.tiff',\n",
       " './train/train_224/43948_right.tiff',\n",
       " './train/train_224/23217_left.tiff',\n",
       " './train/train_224/11543_right.tiff',\n",
       " './train/train_224/39761_left.tiff',\n",
       " './train/train_224/15851_right.tiff',\n",
       " './train/train_224/34719_right.tiff',\n",
       " './train/train_224/21280_left.tiff',\n",
       " './train/train_224/6017_left.tiff',\n",
       " './train/train_224/40639_left.tiff',\n",
       " './train/train_224/15898_left.tiff',\n",
       " './train/train_224/42943_left.tiff',\n",
       " './train/train_224/3453_left.tiff',\n",
       " './train/train_224/16280_left.tiff',\n",
       " './train/train_224/30070_right.tiff',\n",
       " './train/train_224/44159_left.tiff',\n",
       " './train/train_224/9661_left.tiff',\n",
       " './train/train_224/20896_right.tiff',\n",
       " './train/train_224/5250_left.tiff',\n",
       " './train/train_224/6493_right.tiff',\n",
       " './train/train_224/11724_left.tiff',\n",
       " './train/train_224/307_left.tiff',\n",
       " './train/train_224/378_right.tiff',\n",
       " './train/train_224/16099_left.tiff',\n",
       " './train/train_224/27780_left.tiff',\n",
       " './train/train_224/43452_right.tiff',\n",
       " './train/train_224/22112_left.tiff',\n",
       " './train/train_224/29739_right.tiff',\n",
       " './train/train_224/26627_right.tiff',\n",
       " './train/train_224/34382_left.tiff',\n",
       " './train/train_224/36588_left.tiff',\n",
       " './train/train_224/32580_right.tiff',\n",
       " './train/train_224/37378_right.tiff',\n",
       " './train/train_224/8915_right.tiff',\n",
       " './train/train_224/11271_left.tiff',\n",
       " './train/train_224/4400_left.tiff',\n",
       " './train/train_224/19368_right.tiff',\n",
       " './train/train_224/7087_left.tiff',\n",
       " './train/train_224/13584_left.tiff',\n",
       " './train/train_224/3359_left.tiff',\n",
       " './train/train_224/15273_right.tiff',\n",
       " './train/train_224/32015_right.tiff',\n",
       " './train/train_224/37633_right.tiff',\n",
       " './train/train_224/37612_right.tiff',\n",
       " './train/train_224/15274_right.tiff',\n",
       " './train/train_224/13561_right.tiff',\n",
       " './train/train_224/33878_right.tiff',\n",
       " './train/train_224/14387_right.tiff',\n",
       " './train/train_224/9503_right.tiff',\n",
       " './train/train_224/14992_right.tiff',\n",
       " './train/train_224/20330_right.tiff',\n",
       " './train/train_224/32978_left.tiff',\n",
       " './train/train_224/29010_right.tiff',\n",
       " './train/train_224/36213_right.tiff',\n",
       " './train/train_224/26442_left.tiff',\n",
       " './train/train_224/35973_right.tiff',\n",
       " './train/train_224/793_left.tiff',\n",
       " './train/train_224/31812_left.tiff',\n",
       " './train/train_224/22837_left.tiff',\n",
       " './train/train_224/36167_left.tiff',\n",
       " './train/train_224/10785_right.tiff',\n",
       " './train/train_224/22484_right.tiff',\n",
       " './train/train_224/8515_left.tiff',\n",
       " './train/train_224/4559_left.tiff',\n",
       " './train/train_224/27920_right.tiff',\n",
       " './train/train_224/33777_left.tiff',\n",
       " './train/train_224/30638_left.tiff',\n",
       " './train/train_224/23534_left.tiff',\n",
       " './train/train_224/26353_left.tiff',\n",
       " './train/train_224/7222_right.tiff',\n",
       " './train/train_224/40053_right.tiff',\n",
       " './train/train_224/1259_left.tiff',\n",
       " './train/train_224/36966_right.tiff',\n",
       " './train/train_224/10043_left.tiff',\n",
       " './train/train_224/2336_right.tiff',\n",
       " './train/train_224/252_right.tiff',\n",
       " './train/train_224/34748_left.tiff',\n",
       " './train/train_224/18968_right.tiff',\n",
       " './train/train_224/29956_right.tiff',\n",
       " './train/train_224/14251_left.tiff',\n",
       " './train/train_224/7107_right.tiff',\n",
       " './train/train_224/38243_right.tiff',\n",
       " './train/train_224/38959_right.tiff',\n",
       " './train/train_224/23898_right.tiff',\n",
       " './train/train_224/24931_right.tiff',\n",
       " './train/train_224/3992_right.tiff',\n",
       " './train/train_224/16206_left.tiff',\n",
       " './train/train_224/1660_left.tiff',\n",
       " './train/train_224/33605_right.tiff',\n",
       " './train/train_224/6763_left.tiff',\n",
       " './train/train_224/6416_left.tiff',\n",
       " './train/train_224/35281_left.tiff',\n",
       " './train/train_224/7808_right.tiff',\n",
       " './train/train_224/23038_right.tiff',\n",
       " './train/train_224/33912_left.tiff',\n",
       " './train/train_224/878_right.tiff',\n",
       " './train/train_224/43840_left.tiff',\n",
       " './train/train_224/8274_right.tiff',\n",
       " './train/train_224/43921_left.tiff',\n",
       " './train/train_224/24577_right.tiff',\n",
       " './train/train_224/15742_right.tiff',\n",
       " './train/train_224/4946_left.tiff',\n",
       " './train/train_224/15230_right.tiff',\n",
       " './train/train_224/21053_left.tiff',\n",
       " './train/train_224/25462_right.tiff',\n",
       " './train/train_224/16310_left.tiff',\n",
       " './train/train_224/29671_right.tiff',\n",
       " './train/train_224/5396_right.tiff',\n",
       " './train/train_224/23318_right.tiff',\n",
       " './train/train_224/20144_left.tiff',\n",
       " './train/train_224/28416_left.tiff',\n",
       " './train/train_224/17002_left.tiff',\n",
       " './train/train_224/4997_left.tiff',\n",
       " './train/train_224/28320_right.tiff',\n",
       " './train/train_224/22511_right.tiff',\n",
       " './train/train_224/19356_left.tiff',\n",
       " './train/train_224/40224_left.tiff',\n",
       " './train/train_224/38858_left.tiff',\n",
       " './train/train_224/14955_left.tiff',\n",
       " './train/train_224/8645_right.tiff',\n",
       " './train/train_224/4135_left.tiff',\n",
       " './train/train_224/4434_left.tiff',\n",
       " './train/train_224/20977_right.tiff',\n",
       " './train/train_224/21383_left.tiff',\n",
       " './train/train_224/35131_right.tiff',\n",
       " './train/train_224/34810_left.tiff',\n",
       " './train/train_224/1643_right.tiff',\n",
       " './train/train_224/42679_right.tiff',\n",
       " './train/train_224/29019_left.tiff',\n",
       " './train/train_224/7008_left.tiff',\n",
       " './train/train_224/283_right.tiff',\n",
       " './train/train_224/20381_left.tiff',\n",
       " './train/train_224/30094_left.tiff',\n",
       " './train/train_224/23656_right.tiff',\n",
       " './train/train_224/18095_left.tiff',\n",
       " './train/train_224/41365_right.tiff',\n",
       " './train/train_224/43810_right.tiff',\n",
       " './train/train_224/42260_left.tiff',\n",
       " './train/train_224/9957_left.tiff',\n",
       " './train/train_224/31181_left.tiff',\n",
       " './train/train_224/17762_right.tiff',\n",
       " './train/train_224/41748_right.tiff',\n",
       " './train/train_224/13177_right.tiff',\n",
       " './train/train_224/44118_right.tiff',\n",
       " './train/train_224/1346_left.tiff',\n",
       " './train/train_224/16009_right.tiff',\n",
       " './train/train_224/10977_right.tiff',\n",
       " './train/train_224/2776_right.tiff',\n",
       " './train/train_224/21533_left.tiff',\n",
       " './train/train_224/29599_left.tiff',\n",
       " './train/train_224/35726_right.tiff',\n",
       " './train/train_224/4497_right.tiff',\n",
       " './train/train_224/194_left.tiff',\n",
       " './train/train_224/15035_left.tiff',\n",
       " './train/train_224/30345_left.tiff',\n",
       " './train/train_224/16978_left.tiff',\n",
       " './train/train_224/13821_right.tiff',\n",
       " './train/train_224/25019_left.tiff',\n",
       " './train/train_224/33210_right.tiff',\n",
       " './train/train_224/9724_left.tiff',\n",
       " './train/train_224/13013_right.tiff',\n",
       " './train/train_224/36584_left.tiff',\n",
       " './train/train_224/41388_right.tiff',\n",
       " './train/train_224/24880_left.tiff',\n",
       " './train/train_224/38767_right.tiff',\n",
       " './train/train_224/27562_right.tiff',\n",
       " './train/train_224/11143_left.tiff',\n",
       " './train/train_224/42217_left.tiff',\n",
       " './train/train_224/3265_left.tiff',\n",
       " './train/train_224/36965_left.tiff',\n",
       " './train/train_224/3932_right.tiff',\n",
       " './train/train_224/2102_right.tiff',\n",
       " './train/train_224/17736_right.tiff',\n",
       " './train/train_224/25946_right.tiff',\n",
       " './train/train_224/21741_right.tiff',\n",
       " './train/train_224/13190_right.tiff',\n",
       " './train/train_224/23628_left.tiff',\n",
       " './train/train_224/11953_left.tiff',\n",
       " './train/train_224/42709_left.tiff',\n",
       " './train/train_224/30700_right.tiff',\n",
       " './train/train_224/37867_right.tiff',\n",
       " './train/train_224/18231_right.tiff',\n",
       " './train/train_224/24944_left.tiff',\n",
       " './train/train_224/13739_left.tiff',\n",
       " './train/train_224/35616_left.tiff',\n",
       " './train/train_224/9555_left.tiff',\n",
       " './train/train_224/3520_right.tiff',\n",
       " './train/train_224/18420_right.tiff',\n",
       " './train/train_224/16646_left.tiff',\n",
       " './train/train_224/13099_left.tiff',\n",
       " './train/train_224/22774_left.tiff',\n",
       " './train/train_224/27768_left.tiff',\n",
       " './train/train_224/15366_right.tiff',\n",
       " './train/train_224/22872_right.tiff',\n",
       " './train/train_224/8015_right.tiff',\n",
       " './train/train_224/16273_left.tiff',\n",
       " './train/train_224/42194_right.tiff',\n",
       " './train/train_224/783_right.tiff',\n",
       " './train/train_224/5070_right.tiff',\n",
       " './train/train_224/30489_right.tiff',\n",
       " './train/train_224/5035_left.tiff',\n",
       " './train/train_224/6665_right.tiff',\n",
       " './train/train_224/32220_left.tiff',\n",
       " './train/train_224/42414_right.tiff',\n",
       " './train/train_224/25004_left.tiff',\n",
       " './train/train_224/8216_left.tiff',\n",
       " './train/train_224/14807_right.tiff',\n",
       " './train/train_224/27858_left.tiff',\n",
       " './train/train_224/2622_left.tiff',\n",
       " './train/train_224/38072_left.tiff',\n",
       " './train/train_224/24510_right.tiff',\n",
       " './train/train_224/30471_right.tiff',\n",
       " './train/train_224/23827_right.tiff',\n",
       " './train/train_224/26319_right.tiff',\n",
       " './train/train_224/706_right.tiff',\n",
       " './train/train_224/24488_right.tiff',\n",
       " './train/train_224/40723_right.tiff',\n",
       " './train/train_224/33590_right.tiff',\n",
       " './train/train_224/15858_right.tiff',\n",
       " './train/train_224/23783_right.tiff',\n",
       " './train/train_224/7646_left.tiff',\n",
       " './train/train_224/30655_left.tiff',\n",
       " './train/train_224/19762_right.tiff',\n",
       " './train/train_224/22540_right.tiff',\n",
       " './train/train_224/14443_left.tiff',\n",
       " './train/train_224/21194_right.tiff',\n",
       " './train/train_224/12633_right.tiff',\n",
       " './train/train_224/33109_right.tiff',\n",
       " './train/train_224/9966_left.tiff',\n",
       " './train/train_224/31178_left.tiff',\n",
       " './train/train_224/21744_right.tiff',\n",
       " './train/train_224/4632_left.tiff',\n",
       " './train/train_224/34105_right.tiff',\n",
       " './train/train_224/24983_right.tiff',\n",
       " './train/train_224/43294_left.tiff',\n",
       " './train/train_224/42083_right.tiff',\n",
       " './train/train_224/35027_right.tiff',\n",
       " './train/train_224/4390_left.tiff',\n",
       " './train/train_224/9517_left.tiff',\n",
       " './train/train_224/8222_left.tiff',\n",
       " './train/train_224/21042_right.tiff',\n",
       " './train/train_224/21511_left.tiff',\n",
       " './train/train_224/20642_left.tiff',\n",
       " './train/train_224/33460_left.tiff',\n",
       " './train/train_224/21158_right.tiff',\n",
       " './train/train_224/33110_left.tiff',\n",
       " './train/train_224/11855_right.tiff',\n",
       " './train/train_224/9040_left.tiff',\n",
       " './train/train_224/17967_left.tiff',\n",
       " './train/train_224/33585_left.tiff',\n",
       " './train/train_224/19285_left.tiff',\n",
       " './train/train_224/7094_left.tiff',\n",
       " './train/train_224/43074_right.tiff',\n",
       " './train/train_224/39297_left.tiff',\n",
       " './train/train_224/23124_right.tiff',\n",
       " './train/train_224/2081_left.tiff',\n",
       " './train/train_224/31358_right.tiff',\n",
       " './train/train_224/36730_right.tiff',\n",
       " './train/train_224/23094_left.tiff',\n",
       " './train/train_224/1495_left.tiff',\n",
       " './train/train_224/31216_right.tiff',\n",
       " './train/train_224/23431_right.tiff',\n",
       " './train/train_224/22214_left.tiff',\n",
       " './train/train_224/7290_left.tiff',\n",
       " './train/train_224/34690_left.tiff',\n",
       " './train/train_224/38998_right.tiff',\n",
       " './train/train_224/7437_left.tiff',\n",
       " './train/train_224/26784_left.tiff',\n",
       " './train/train_224/14455_left.tiff',\n",
       " './train/train_224/33762_right.tiff',\n",
       " './train/train_224/43813_left.tiff',\n",
       " './train/train_224/27715_right.tiff',\n",
       " './train/train_224/23994_right.tiff',\n",
       " './train/train_224/1396_right.tiff',\n",
       " './train/train_224/11519_left.tiff',\n",
       " './train/train_224/4404_right.tiff',\n",
       " './train/train_224/20591_left.tiff',\n",
       " './train/train_224/25535_right.tiff',\n",
       " './train/train_224/23553_left.tiff',\n",
       " './train/train_224/27625_left.tiff',\n",
       " './train/train_224/27228_right.tiff',\n",
       " './train/train_224/41843_right.tiff',\n",
       " './train/train_224/21960_left.tiff',\n",
       " './train/train_224/3779_left.tiff',\n",
       " './train/train_224/41599_left.tiff',\n",
       " './train/train_224/35859_right.tiff',\n",
       " './train/train_224/16969_left.tiff',\n",
       " './train/train_224/10975_left.tiff',\n",
       " './train/train_224/33894_left.tiff',\n",
       " './train/train_224/5192_left.tiff',\n",
       " './train/train_224/17025_right.tiff',\n",
       " './train/train_224/3355_right.tiff',\n",
       " './train/train_224/28290_right.tiff',\n",
       " './train/train_224/2423_left.tiff',\n",
       " './train/train_224/8026_left.tiff',\n",
       " './train/train_224/20167_left.tiff',\n",
       " './train/train_224/40868_left.tiff',\n",
       " './train/train_224/41445_right.tiff',\n",
       " './train/train_224/18573_right.tiff',\n",
       " './train/train_224/20522_left.tiff',\n",
       " './train/train_224/35610_left.tiff',\n",
       " './train/train_224/41119_left.tiff',\n",
       " './train/train_224/16936_right.tiff',\n",
       " './train/train_224/36234_left.tiff',\n",
       " './train/train_224/36148_right.tiff',\n",
       " './train/train_224/29976_right.tiff',\n",
       " './train/train_224/19130_left.tiff',\n",
       " './train/train_224/5530_right.tiff',\n",
       " './train/train_224/32581_right.tiff',\n",
       " './train/train_224/11273_left.tiff',\n",
       " './train/train_224/27054_left.tiff',\n",
       " './train/train_224/13938_left.tiff',\n",
       " './train/train_224/23395_left.tiff',\n",
       " './train/train_224/8877_right.tiff',\n",
       " './train/train_224/43770_left.tiff',\n",
       " './train/train_224/35052_left.tiff',\n",
       " './train/train_224/40184_right.tiff',\n",
       " './train/train_224/13432_left.tiff',\n",
       " './train/train_224/8348_right.tiff',\n",
       " './train/train_224/15661_left.tiff',\n",
       " './train/train_224/24197_left.tiff',\n",
       " './train/train_224/13541_left.tiff',\n",
       " './train/train_224/7980_left.tiff',\n",
       " './train/train_224/4805_left.tiff',\n",
       " './train/train_224/5500_left.tiff',\n",
       " './train/train_224/33601_left.tiff',\n",
       " './train/train_224/43041_right.tiff',\n",
       " './train/train_224/15594_left.tiff',\n",
       " './train/train_224/42165_left.tiff',\n",
       " './train/train_224/1659_left.tiff',\n",
       " './train/train_224/44231_right.tiff',\n",
       " './train/train_224/22473_left.tiff',\n",
       " './train/train_224/17945_left.tiff',\n",
       " './train/train_224/43540_left.tiff',\n",
       " './train/train_224/34713_left.tiff',\n",
       " './train/train_224/26389_right.tiff',\n",
       " './train/train_224/11221_right.tiff',\n",
       " './train/train_224/1881_left.tiff',\n",
       " './train/train_224/12258_left.tiff',\n",
       " './train/train_224/5732_left.tiff',\n",
       " './train/train_224/23854_left.tiff',\n",
       " './train/train_224/22257_left.tiff',\n",
       " './train/train_224/23798_left.tiff',\n",
       " './train/train_224/15699_left.tiff',\n",
       " './train/train_224/987_right.tiff',\n",
       " './train/train_224/29671_left.tiff',\n",
       " './train/train_224/27203_right.tiff',\n",
       " './train/train_224/28646_right.tiff',\n",
       " './train/train_224/7417_left.tiff',\n",
       " './train/train_224/30360_right.tiff',\n",
       " './train/train_224/22138_right.tiff',\n",
       " './train/train_224/25310_right.tiff',\n",
       " './train/train_224/25458_left.tiff',\n",
       " './train/train_224/37055_right.tiff',\n",
       " './train/train_224/9404_left.tiff',\n",
       " './train/train_224/32070_left.tiff',\n",
       " './train/train_224/32637_left.tiff',\n",
       " './train/train_224/27308_left.tiff',\n",
       " './train/train_224/2812_left.tiff',\n",
       " './train/train_224/35130_left.tiff',\n",
       " './train/train_224/39160_right.tiff',\n",
       " './train/train_224/7470_right.tiff',\n",
       " './train/train_224/15255_right.tiff',\n",
       " './train/train_224/24831_right.tiff',\n",
       " './train/train_224/15074_right.tiff',\n",
       " './train/train_224/5813_left.tiff',\n",
       " './train/train_224/43039_left.tiff',\n",
       " './train/train_224/29042_right.tiff',\n",
       " './train/train_224/1776_left.tiff',\n",
       " './train/train_224/40608_left.tiff',\n",
       " './train/train_224/29751_left.tiff',\n",
       " './train/train_224/16702_right.tiff',\n",
       " './train/train_224/24535_right.tiff',\n",
       " './train/train_224/22684_right.tiff',\n",
       " './train/train_224/4162_left.tiff',\n",
       " './train/train_224/7608_right.tiff',\n",
       " './train/train_224/43348_left.tiff',\n",
       " './train/train_224/34606_left.tiff',\n",
       " './train/train_224/23551_right.tiff',\n",
       " './train/train_224/8339_right.tiff',\n",
       " './train/train_224/1192_right.tiff',\n",
       " './train/train_224/36635_left.tiff',\n",
       " './train/train_224/32079_right.tiff',\n",
       " './train/train_224/11512_right.tiff',\n",
       " './train/train_224/11553_left.tiff',\n",
       " './train/train_224/41622_left.tiff',\n",
       " './train/train_224/4452_right.tiff',\n",
       " './train/train_224/37677_left.tiff',\n",
       " './train/train_224/22210_left.tiff',\n",
       " './train/train_224/4443_left.tiff',\n",
       " './train/train_224/37215_left.tiff',\n",
       " './train/train_224/8126_right.tiff',\n",
       " './train/train_224/34778_right.tiff',\n",
       " './train/train_224/40498_right.tiff',\n",
       " './train/train_224/35151_right.tiff',\n",
       " './train/train_224/8221_right.tiff',\n",
       " './train/train_224/16670_left.tiff',\n",
       " './train/train_224/14987_left.tiff',\n",
       " './train/train_224/25330_left.tiff',\n",
       " './train/train_224/11464_right.tiff',\n",
       " './train/train_224/34994_left.tiff',\n",
       " './train/train_224/36426_right.tiff',\n",
       " './train/train_224/35183_right.tiff',\n",
       " './train/train_224/1092_left.tiff',\n",
       " './train/train_224/23540_right.tiff',\n",
       " './train/train_224/38555_right.tiff',\n",
       " './train/train_224/2405_left.tiff',\n",
       " './train/train_224/38578_right.tiff',\n",
       " './train/train_224/3525_right.tiff',\n",
       " './train/train_224/18300_right.tiff',\n",
       " './train/train_224/40049_left.tiff',\n",
       " './train/train_224/30322_right.tiff',\n",
       " './train/train_224/7391_left.tiff',\n",
       " './train/train_224/21655_left.tiff',\n",
       " './train/train_224/43042_left.tiff',\n",
       " './train/train_224/39711_right.tiff',\n",
       " './train/train_224/17341_right.tiff',\n",
       " './train/train_224/20145_right.tiff',\n",
       " './train/train_224/35594_right.tiff',\n",
       " './train/train_224/5332_left.tiff',\n",
       " './train/train_224/36035_right.tiff',\n",
       " './train/train_224/42586_right.tiff',\n",
       " './train/train_224/26952_right.tiff',\n",
       " './train/train_224/13276_right.tiff',\n",
       " './train/train_224/13229_right.tiff',\n",
       " './train/train_224/39117_left.tiff',\n",
       " './train/train_224/7393_left.tiff',\n",
       " './train/train_224/5538_left.tiff',\n",
       " './train/train_224/17389_left.tiff',\n",
       " './train/train_224/7996_right.tiff',\n",
       " './train/train_224/37346_left.tiff',\n",
       " './train/train_224/22593_left.tiff',\n",
       " './train/train_224/5316_right.tiff',\n",
       " './train/train_224/42705_left.tiff',\n",
       " './train/train_224/22765_right.tiff',\n",
       " './train/train_224/12026_right.tiff',\n",
       " './train/train_224/14013_right.tiff',\n",
       " './train/train_224/33551_right.tiff',\n",
       " './train/train_224/36099_right.tiff',\n",
       " './train/train_224/10923_right.tiff',\n",
       " './train/train_224/1513_right.tiff',\n",
       " './train/train_224/39764_left.tiff',\n",
       " './train/train_224/28281_left.tiff',\n",
       " './train/train_224/11871_left.tiff',\n",
       " './train/train_224/9051_right.tiff',\n",
       " './train/train_224/37728_left.tiff',\n",
       " './train/train_224/10501_left.tiff',\n",
       " './train/train_224/28752_left.tiff',\n",
       " './train/train_224/9798_right.tiff',\n",
       " './train/train_224/29082_left.tiff',\n",
       " './train/train_224/22428_right.tiff',\n",
       " './train/train_224/4755_right.tiff',\n",
       " './train/train_224/13230_right.tiff',\n",
       " './train/train_224/26758_right.tiff',\n",
       " './train/train_224/25717_left.tiff',\n",
       " './train/train_224/42916_left.tiff',\n",
       " './train/train_224/20066_left.tiff',\n",
       " './train/train_224/22193_left.tiff',\n",
       " './train/train_224/11370_left.tiff',\n",
       " './train/train_224/32791_right.tiff',\n",
       " './train/train_224/19442_right.tiff',\n",
       " './train/train_224/32121_left.tiff',\n",
       " './train/train_224/33731_right.tiff',\n",
       " './train/train_224/4070_right.tiff',\n",
       " './train/train_224/8598_left.tiff',\n",
       " './train/train_224/8889_right.tiff',\n",
       " './train/train_224/37568_left.tiff',\n",
       " './train/train_224/21161_right.tiff',\n",
       " './train/train_224/43660_right.tiff',\n",
       " './train/train_224/27939_right.tiff',\n",
       " './train/train_224/42079_right.tiff',\n",
       " './train/train_224/3846_right.tiff',\n",
       " './train/train_224/13596_left.tiff',\n",
       " './train/train_224/258_right.tiff',\n",
       " './train/train_224/18783_left.tiff',\n",
       " './train/train_224/23525_right.tiff',\n",
       " './train/train_224/36071_left.tiff',\n",
       " './train/train_224/40821_right.tiff',\n",
       " './train/train_224/34932_left.tiff',\n",
       " './train/train_224/40731_right.tiff',\n",
       " './train/train_224/18301_right.tiff',\n",
       " './train/train_224/11736_left.tiff',\n",
       " './train/train_224/30689_right.tiff',\n",
       " './train/train_224/899_left.tiff',\n",
       " './train/train_224/36940_right.tiff',\n",
       " './train/train_224/43304_left.tiff',\n",
       " './train/train_224/34203_left.tiff',\n",
       " './train/train_224/22131_right.tiff',\n",
       " './train/train_224/29325_left.tiff',\n",
       " './train/train_224/28129_right.tiff',\n",
       " './train/train_224/21445_left.tiff',\n",
       " './train/train_224/30376_right.tiff',\n",
       " './train/train_224/27127_right.tiff',\n",
       " './train/train_224/39582_right.tiff',\n",
       " './train/train_224/1793_left.tiff',\n",
       " './train/train_224/44146_right.tiff',\n",
       " './train/train_224/405_left.tiff',\n",
       " './train/train_224/39823_left.tiff',\n",
       " './train/train_224/1849_right.tiff',\n",
       " './train/train_224/4212_right.tiff',\n",
       " './train/train_224/41889_right.tiff',\n",
       " './train/train_224/21525_left.tiff',\n",
       " './train/train_224/37405_right.tiff',\n",
       " './train/train_224/39401_right.tiff',\n",
       " './train/train_224/3914_right.tiff',\n",
       " './train/train_224/29027_left.tiff',\n",
       " './train/train_224/8116_right.tiff',\n",
       " './train/train_224/24779_left.tiff',\n",
       " './train/train_224/40984_left.tiff',\n",
       " './train/train_224/17217_right.tiff',\n",
       " './train/train_224/43747_right.tiff',\n",
       " './train/train_224/35145_right.tiff',\n",
       " './train/train_224/34822_right.tiff',\n",
       " './train/train_224/20546_right.tiff',\n",
       " './train/train_224/19458_left.tiff',\n",
       " './train/train_224/25650_left.tiff',\n",
       " './train/train_224/14654_right.tiff',\n",
       " './train/train_224/13178_left.tiff',\n",
       " './train/train_224/24252_right.tiff',\n",
       " './train/train_224/16908_left.tiff',\n",
       " './train/train_224/36047_left.tiff',\n",
       " './train/train_224/17232_right.tiff',\n",
       " './train/train_224/10028_left.tiff',\n",
       " './train/train_224/34997_right.tiff',\n",
       " './train/train_224/27181_left.tiff',\n",
       " './train/train_224/17750_left.tiff',\n",
       " './train/train_224/3370_left.tiff',\n",
       " './train/train_224/21482_right.tiff',\n",
       " './train/train_224/40758_left.tiff',\n",
       " './train/train_224/3655_left.tiff',\n",
       " './train/train_224/9135_right.tiff',\n",
       " './train/train_224/34560_right.tiff',\n",
       " './train/train_224/8806_right.tiff',\n",
       " './train/train_224/18438_left.tiff',\n",
       " './train/train_224/23085_left.tiff',\n",
       " './train/train_224/24641_left.tiff',\n",
       " './train/train_224/29076_right.tiff',\n",
       " './train/train_224/28174_left.tiff',\n",
       " './train/train_224/41852_left.tiff',\n",
       " './train/train_224/14811_right.tiff',\n",
       " './train/train_224/31145_left.tiff',\n",
       " './train/train_224/5295_right.tiff',\n",
       " './train/train_224/20965_right.tiff',\n",
       " './train/train_224/19858_left.tiff',\n",
       " './train/train_224/21856_left.tiff',\n",
       " './train/train_224/18570_right.tiff',\n",
       " './train/train_224/22747_right.tiff',\n",
       " './train/train_224/396_right.tiff',\n",
       " './train/train_224/7606_left.tiff',\n",
       " './train/train_224/1431_right.tiff',\n",
       " './train/train_224/10029_right.tiff',\n",
       " './train/train_224/37630_right.tiff',\n",
       " './train/train_224/32010_left.tiff',\n",
       " './train/train_224/41836_left.tiff',\n",
       " './train/train_224/34407_right.tiff',\n",
       " './train/train_224/7690_left.tiff',\n",
       " './train/train_224/35166_left.tiff',\n",
       " './train/train_224/31000_left.tiff',\n",
       " './train/train_224/29352_left.tiff',\n",
       " './train/train_224/39758_left.tiff',\n",
       " './train/train_224/5579_right.tiff',\n",
       " './train/train_224/4139_right.tiff',\n",
       " './train/train_224/27781_right.tiff',\n",
       " './train/train_224/16582_left.tiff',\n",
       " './train/train_224/8262_right.tiff',\n",
       " './train/train_224/10426_right.tiff',\n",
       " './train/train_224/42702_right.tiff',\n",
       " './train/train_224/16413_left.tiff',\n",
       " './train/train_224/1727_left.tiff',\n",
       " './train/train_224/4857_right.tiff',\n",
       " './train/train_224/31014_left.tiff',\n",
       " './train/train_224/3641_left.tiff',\n",
       " './train/train_224/36246_right.tiff',\n",
       " './train/train_224/41807_left.tiff',\n",
       " './train/train_224/18972_right.tiff',\n",
       " './train/train_224/39915_left.tiff',\n",
       " './train/train_224/17636_right.tiff',\n",
       " './train/train_224/38466_right.tiff',\n",
       " './train/train_224/39577_right.tiff',\n",
       " './train/train_224/24107_right.tiff',\n",
       " './train/train_224/25438_right.tiff',\n",
       " './train/train_224/29711_right.tiff',\n",
       " './train/train_224/11600_right.tiff',\n",
       " './train/train_224/38914_right.tiff',\n",
       " './train/train_224/22624_left.tiff',\n",
       " './train/train_224/26836_right.tiff',\n",
       " './train/train_224/32744_left.tiff',\n",
       " './train/train_224/31035_left.tiff',\n",
       " './train/train_224/26692_left.tiff',\n",
       " './train/train_224/28181_right.tiff',\n",
       " './train/train_224/39783_left.tiff',\n",
       " './train/train_224/23593_left.tiff',\n",
       " './train/train_224/2430_right.tiff',\n",
       " './train/train_224/14154_left.tiff',\n",
       " './train/train_224/29619_left.tiff',\n",
       " './train/train_224/17054_right.tiff',\n",
       " './train/train_224/13629_right.tiff',\n",
       " './train/train_224/40386_right.tiff',\n",
       " './train/train_224/24368_right.tiff',\n",
       " './train/train_224/43815_left.tiff',\n",
       " './train/train_224/29737_right.tiff',\n",
       " './train/train_224/4064_left.tiff',\n",
       " './train/train_224/39583_left.tiff',\n",
       " './train/train_224/40409_left.tiff',\n",
       " './train/train_224/30733_left.tiff',\n",
       " './train/train_224/37939_left.tiff',\n",
       " './train/train_224/19043_left.tiff',\n",
       " './train/train_224/7352_right.tiff',\n",
       " './train/train_224/6700_right.tiff',\n",
       " './train/train_224/4649_left.tiff',\n",
       " './train/train_224/42701_left.tiff',\n",
       " './train/train_224/30912_left.tiff',\n",
       " './train/train_224/25763_left.tiff',\n",
       " './train/train_224/24909_right.tiff',\n",
       " './train/train_224/30597_left.tiff',\n",
       " './train/train_224/13923_left.tiff',\n",
       " './train/train_224/16132_left.tiff',\n",
       " './train/train_224/4416_left.tiff',\n",
       " './train/train_224/34668_left.tiff',\n",
       " './train/train_224/44221_left.tiff',\n",
       " './train/train_224/10675_left.tiff',\n",
       " './train/train_224/40723_left.tiff',\n",
       " './train/train_224/25154_right.tiff',\n",
       " './train/train_224/40247_right.tiff',\n",
       " './train/train_224/30236_right.tiff',\n",
       " './train/train_224/43894_right.tiff',\n",
       " './train/train_224/41198_left.tiff',\n",
       " './train/train_224/19180_right.tiff',\n",
       " './train/train_224/11395_right.tiff',\n",
       " './train/train_224/10131_right.tiff',\n",
       " './train/train_224/25728_right.tiff',\n",
       " './train/train_224/15114_left.tiff',\n",
       " './train/train_224/8242_left.tiff',\n",
       " './train/train_224/40889_left.tiff',\n",
       " './train/train_224/7351_left.tiff',\n",
       " './train/train_224/34038_left.tiff',\n",
       " './train/train_224/41714_right.tiff',\n",
       " './train/train_224/34828_left.tiff',\n",
       " './train/train_224/301_right.tiff',\n",
       " './train/train_224/40050_left.tiff',\n",
       " './train/train_224/1362_left.tiff',\n",
       " './train/train_224/17183_right.tiff',\n",
       " './train/train_224/5468_right.tiff',\n",
       " './train/train_224/24138_right.tiff',\n",
       " './train/train_224/27496_right.tiff',\n",
       " './train/train_224/42949_left.tiff',\n",
       " './train/train_224/30725_right.tiff',\n",
       " './train/train_224/8484_right.tiff',\n",
       " './train/train_224/39398_left.tiff',\n",
       " './train/train_224/15937_right.tiff',\n",
       " './train/train_224/16109_right.tiff',\n",
       " './train/train_224/18744_right.tiff',\n",
       " './train/train_224/16877_left.tiff',\n",
       " './train/train_224/6997_right.tiff',\n",
       " './train/train_224/43872_right.tiff',\n",
       " './train/train_224/11231_right.tiff',\n",
       " ...]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"train_idx\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in images_train_list:\n",
    "    fname = file.split(\"/\")[-1]\n",
    "    f.write(fname+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
